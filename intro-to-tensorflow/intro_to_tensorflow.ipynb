{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">TensorFlow Neural Network Lab</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/notmnist.png\">\n",
    "In this lab, you'll use all the tools you learned from *Introduction to TensorFlow* to label images of English letters! The data you are using, <a href=\"http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html\">notMNIST</a>, consists of images of a letter from A to J in different fonts.\n",
    "\n",
    "The above images are a few examples of the data you'll be training on. After training the network, you will compare your prediction model against test data. Your goal, by the end of this lab, is to make predictions against that test set with at least an 80% accuracy. Let's jump in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start this lab, you first need to import all the necessary modules. Run the code below. If it runs successfully, it will print \"`All modules imported`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import resample\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "\n",
    "print('All modules imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notMNIST dataset is too large for many computers to handle.  It contains 500,000 images for just training.  You'll be using a subset of this data, 15,000 images for each label (A-J)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded.\n"
     ]
    }
   ],
   "source": [
    "def download(url, file):\n",
    "    \"\"\"\n",
    "    Download file from <url>\n",
    "    :param url: URL to file\n",
    "    :param file: Local file path\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file):\n",
    "        print('Downloading ' + file + '...')\n",
    "        urlretrieve(url, file)\n",
    "        print('Download Finished')\n",
    "\n",
    "# Download the training and test dataset.\n",
    "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_train.zip', 'notMNIST_train.zip')\n",
    "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_test.zip', 'notMNIST_test.zip')\n",
    "\n",
    "# Make sure the files aren't corrupted\n",
    "assert hashlib.md5(open('notMNIST_train.zip', 'rb').read()).hexdigest() == 'c8673b3f28f489e9cdf3a3d74e2ac8fa',\\\n",
    "        'notMNIST_train.zip file is corrupted.  Remove the file and try again.'\n",
    "assert hashlib.md5(open('notMNIST_test.zip', 'rb').read()).hexdigest() == '5d3c7e653e63471c88df796156a9dfa9',\\\n",
    "        'notMNIST_test.zip file is corrupted.  Remove the file and try again.'\n",
    "\n",
    "# Wait until you see that all files have been downloaded.\n",
    "print('All files downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/210001 [00:00<?, ?files/s]\u001b[A\n",
      "  0%|          | 2/210001 [00:00<3:14:50, 17.96files/s]\u001b[A\n",
      "  0%|          | 363/210001 [00:00<02:01, 1719.89files/s]\u001b[A\n",
      "  0%|          | 760/210001 [00:00<01:25, 2441.07files/s]\u001b[A\n",
      "  1%|          | 1186/210001 [00:00<01:12, 2882.34files/s]\u001b[A\n",
      "  1%|          | 1592/210001 [00:00<01:07, 3109.06files/s]\u001b[A\n",
      "  1%|          | 2006/210001 [00:00<01:03, 3278.43files/s]\u001b[A\n",
      "  1%|          | 2405/210001 [00:00<01:01, 3378.09files/s]\u001b[A\n",
      "  1%|▏         | 2835/210001 [00:00<00:59, 3491.44files/s]\u001b[A\n",
      "  2%|▏         | 3270/210001 [00:00<00:57, 3584.48files/s]\u001b[A\n",
      "  2%|▏         | 3708/210001 [00:01<00:56, 3662.83files/s]\u001b[A\n",
      "  2%|▏         | 4145/210001 [00:01<00:55, 3726.13files/s]\u001b[A\n",
      "  2%|▏         | 4591/210001 [00:01<00:54, 3787.34files/s]\u001b[A\n",
      "  2%|▏         | 5034/210001 [00:01<00:53, 3835.90files/s]\u001b[A\n",
      "  3%|▎         | 5465/210001 [00:01<00:52, 3859.96files/s]\u001b[A\n",
      "  3%|▎         | 5892/210001 [00:01<00:52, 3852.60files/s]\u001b[A\n",
      "  3%|▎         | 6361/210001 [00:01<00:52, 3903.87files/s]\u001b[A\n",
      "  3%|▎         | 6811/210001 [00:01<00:51, 3938.18files/s]\u001b[A\n",
      "  3%|▎         | 7246/210001 [00:01<00:51, 3960.53files/s]\u001b[A\n",
      "  4%|▎         | 7681/210001 [00:01<00:50, 3977.11files/s]\u001b[A\n",
      "  4%|▍         | 8114/210001 [00:02<00:50, 3990.43files/s]\u001b[A\n",
      "  4%|▍         | 8572/210001 [00:02<00:50, 4018.14files/s]\u001b[A\n",
      "  4%|▍         | 9032/210001 [00:02<00:49, 4043.84files/s]\u001b[A\n",
      "  5%|▍         | 9479/210001 [00:02<00:49, 4061.82files/s]\u001b[A\n",
      "  5%|▍         | 9934/210001 [00:02<00:49, 4081.79files/s]\u001b[A\n",
      "  5%|▍         | 10383/210001 [00:02<00:48, 4092.91files/s]\u001b[A\n",
      "  5%|▌         | 10828/210001 [00:02<00:48, 4103.92files/s]\u001b[A\n",
      "  5%|▌         | 11271/210001 [00:02<00:48, 4115.68files/s]\u001b[A\n",
      "  6%|▌         | 11714/210001 [00:02<00:48, 4125.34files/s]\u001b[A\n",
      "  6%|▌         | 12181/210001 [00:02<00:47, 4143.49files/s]\u001b[A\n",
      "  6%|▌         | 12630/210001 [00:03<00:47, 4151.45files/s]\u001b[A\n",
      "  6%|▌         | 13089/210001 [00:03<00:47, 4164.95files/s]\u001b[A\n",
      "  6%|▋         | 13550/210001 [00:03<00:47, 4178.50files/s]\u001b[A\n",
      "  7%|▋         | 14003/210001 [00:03<00:46, 4187.02files/s]\u001b[A\n",
      "  7%|▋         | 14454/210001 [00:03<00:46, 4194.20files/s]\u001b[A\n",
      "  7%|▋         | 14928/210001 [00:03<00:46, 4209.36files/s]\u001b[A\n",
      "  7%|▋         | 15384/210001 [00:03<00:46, 4216.31files/s]\u001b[A\n",
      "  8%|▊         | 15845/210001 [00:03<00:45, 4226.85files/s]\u001b[A\n",
      "  8%|▊         | 16301/210001 [00:03<00:45, 4231.09files/s]\u001b[A\n",
      "  8%|▊         | 16752/210001 [00:03<00:45, 4234.93files/s]\u001b[A\n",
      "  8%|▊         | 17199/210001 [00:04<00:45, 4239.09files/s]\u001b[A\n",
      "  8%|▊         | 17652/210001 [00:04<00:45, 4245.81files/s]\u001b[A\n",
      "  9%|▊         | 18100/210001 [00:04<00:45, 4246.39files/s]\u001b[A\n",
      "  9%|▉         | 18541/210001 [00:04<00:45, 4245.87files/s]\u001b[A\n",
      "  9%|▉         | 18982/210001 [00:04<00:44, 4249.35files/s]\u001b[A\n",
      "  9%|▉         | 19419/210001 [00:04<00:44, 4239.94files/s]\u001b[A\n",
      "  9%|▉         | 19841/210001 [00:04<00:44, 4230.47files/s]\u001b[A\n",
      " 10%|▉         | 20258/210001 [00:04<00:44, 4228.95files/s]\u001b[A\n",
      " 10%|▉         | 20685/210001 [00:04<00:44, 4229.77files/s]\u001b[A\n",
      " 10%|█         | 21139/210001 [00:04<00:44, 4235.92files/s]\u001b[A\n",
      " 10%|█         | 21567/210001 [00:05<00:44, 4236.16files/s]\u001b[A\n",
      " 10%|█         | 21994/210001 [00:05<00:44, 4228.09files/s]\u001b[A\n",
      " 11%|█         | 22409/210001 [00:05<00:44, 4206.04files/s]\u001b[A\n",
      " 11%|█         | 22798/210001 [00:05<00:44, 4197.22files/s]\u001b[A\n",
      " 11%|█         | 23183/210001 [00:05<00:44, 4175.30files/s]\u001b[A\n",
      " 11%|█         | 23615/210001 [00:05<00:44, 4177.86files/s]\u001b[A\n",
      " 11%|█▏        | 24027/210001 [00:05<00:44, 4176.06files/s]\u001b[A\n",
      " 12%|█▏        | 24420/210001 [00:05<00:44, 4165.74files/s]\u001b[A\n",
      " 12%|█▏        | 24803/210001 [00:05<00:44, 4157.61files/s]\u001b[A\n",
      " 12%|█▏        | 25231/210001 [00:06<00:44, 4159.57files/s]\u001b[A\n",
      " 12%|█▏        | 25652/210001 [00:06<00:44, 4160.21files/s]\u001b[A\n",
      " 12%|█▏        | 26072/210001 [00:06<00:44, 4160.84files/s]\u001b[A\n",
      " 13%|█▎        | 26496/210001 [00:06<00:44, 4162.03files/s]\u001b[A\n",
      " 13%|█▎        | 26908/210001 [00:06<00:44, 4156.74files/s]\u001b[A\n",
      " 13%|█▎        | 27317/210001 [00:06<00:43, 4155.41files/s]\u001b[A\n",
      " 13%|█▎        | 27722/210001 [00:06<00:43, 4152.25files/s]\u001b[A\n",
      " 13%|█▎        | 28159/210001 [00:06<00:43, 4155.40files/s]\u001b[A\n",
      " 14%|█▎        | 28589/210001 [00:06<00:43, 4157.05files/s]\u001b[A\n",
      " 14%|█▍        | 29007/210001 [00:06<00:43, 4154.22files/s]\u001b[A\n",
      " 14%|█▍        | 29444/210001 [00:07<00:43, 4157.14files/s]\u001b[A\n",
      " 14%|█▍        | 29863/210001 [00:07<00:43, 4157.34files/s]\u001b[A\n",
      " 14%|█▍        | 30281/210001 [00:07<00:43, 4157.23files/s]\u001b[A\n",
      " 15%|█▍        | 30715/210001 [00:07<00:43, 4159.44files/s]\u001b[A\n",
      " 15%|█▍        | 31137/210001 [00:07<00:43, 4155.18files/s]\u001b[A\n",
      " 15%|█▌        | 31549/210001 [00:07<00:42, 4153.43files/s]\u001b[A\n",
      " 15%|█▌        | 31964/210001 [00:07<00:42, 4153.34files/s]\u001b[A\n",
      " 15%|█▌        | 32401/210001 [00:07<00:42, 4155.99files/s]\u001b[A\n",
      " 16%|█▌        | 32823/210001 [00:07<00:42, 4156.73files/s]\u001b[A\n",
      " 16%|█▌        | 33254/210001 [00:07<00:42, 4158.42files/s]\u001b[A\n",
      " 16%|█▌        | 33685/210001 [00:08<00:42, 4160.34files/s]\u001b[A\n",
      " 16%|█▌        | 34110/210001 [00:08<00:42, 4160.09files/s]\u001b[A\n",
      " 16%|█▋        | 34542/210001 [00:08<00:42, 4162.04files/s]\u001b[A\n",
      " 17%|█▋        | 34997/210001 [00:08<00:42, 4166.43files/s]\u001b[A\n",
      " 17%|█▋        | 35431/210001 [00:08<00:41, 4167.57files/s]\u001b[A\n",
      " 17%|█▋        | 35862/210001 [00:08<00:41, 4166.14files/s]\u001b[A\n",
      " 17%|█▋        | 36312/210001 [00:08<00:41, 4169.98files/s]\u001b[A\n",
      " 17%|█▋        | 36744/210001 [00:08<00:41, 4171.42files/s]\u001b[A\n",
      " 18%|█▊        | 37181/210001 [00:08<00:41, 4173.60files/s]\u001b[A\n",
      " 18%|█▊        | 37614/210001 [00:09<00:41, 4174.56files/s]\u001b[A\n",
      " 18%|█▊        | 38045/210001 [00:09<00:41, 4175.19files/s]\u001b[A\n",
      " 18%|█▊        | 38496/210001 [00:09<00:41, 4178.87files/s]\u001b[A\n",
      " 19%|█▊        | 38934/210001 [00:09<00:40, 4180.95files/s]\u001b[A\n",
      " 19%|█▊        | 39371/210001 [00:09<00:40, 4182.91files/s]\u001b[A\n",
      " 19%|█▉        | 39807/210001 [00:09<00:40, 4178.69files/s]\u001b[A\n",
      " 19%|█▉        | 40228/210001 [00:09<00:40, 4178.54files/s]\u001b[A\n",
      " 19%|█▉        | 40660/210001 [00:09<00:40, 4179.73files/s]\u001b[A\n",
      " 20%|█▉        | 41110/210001 [00:09<00:40, 4182.94files/s]\u001b[A\n",
      " 20%|█▉        | 41546/210001 [00:09<00:40, 4184.82files/s]\u001b[A\n",
      " 20%|█▉        | 41994/210001 [00:10<00:40, 4187.62files/s]\u001b[A\n",
      " 20%|██        | 42465/210001 [00:10<00:39, 4192.70files/s]\u001b[A\n",
      " 20%|██        | 42922/210001 [00:10<00:39, 4196.41files/s]\u001b[A\n",
      " 21%|██        | 43372/210001 [00:10<00:39, 4199.04files/s]\u001b[A\n",
      " 21%|██        | 43821/210001 [00:10<00:39, 4199.97files/s]\u001b[A\n",
      " 21%|██        | 44264/210001 [00:10<00:39, 4197.39files/s]\u001b[A\n",
      " 21%|██▏       | 44693/210001 [00:10<00:39, 4195.03files/s]\u001b[A\n",
      " 21%|██▏       | 45113/210001 [00:10<00:39, 4193.87files/s]\u001b[A\n",
      " 22%|██▏       | 45584/210001 [00:10<00:39, 4198.63files/s]\u001b[A\n",
      " 22%|██▏       | 46051/210001 [00:10<00:39, 4203.02files/s]\u001b[A\n",
      " 22%|██▏       | 46519/210001 [00:11<00:38, 4207.21files/s]\u001b[A\n",
      " 22%|██▏       | 46970/210001 [00:11<00:38, 4209.79files/s]\u001b[A\n",
      " 23%|██▎       | 47420/210001 [00:11<00:38, 4209.06files/s]\u001b[A\n",
      " 23%|██▎       | 47859/210001 [00:11<00:38, 4208.83files/s]\u001b[A\n",
      " 23%|██▎       | 48292/210001 [00:11<00:38, 4209.68files/s]\u001b[A\n",
      " 23%|██▎       | 48724/210001 [00:11<00:38, 4209.48files/s]\u001b[A\n",
      " 23%|██▎       | 49184/210001 [00:11<00:38, 4212.71files/s]\u001b[A\n",
      " 24%|██▎       | 49638/210001 [00:11<00:38, 4215.50files/s]\u001b[A\n",
      " 24%|██▍       | 50084/210001 [00:11<00:37, 4217.52files/s]\u001b[A\n",
      " 24%|██▍       | 50559/210001 [00:11<00:37, 4221.91files/s]\u001b[A\n",
      " 24%|██▍       | 51015/210001 [00:12<00:37, 4224.68files/s]\u001b[A\n",
      " 25%|██▍       | 51469/210001 [00:12<00:37, 4224.99files/s]\u001b[A\n",
      " 25%|██▍       | 51939/210001 [00:12<00:37, 4228.78files/s]\u001b[A\n",
      " 25%|██▍       | 52408/210001 [00:12<00:37, 4232.54files/s]\u001b[A\n",
      " 25%|██▌       | 52868/210001 [00:12<00:37, 4235.37files/s]\u001b[A\n",
      " 25%|██▌       | 53349/210001 [00:12<00:36, 4239.92files/s]\u001b[A\n",
      " 26%|██▌       | 53814/210001 [00:12<00:36, 4241.81files/s]\u001b[A\n",
      " 26%|██▌       | 54281/210001 [00:12<00:36, 4244.94files/s]\u001b[A\n",
      " 26%|██▌       | 54742/210001 [00:12<00:36, 4246.77files/s]\u001b[A\n",
      " 26%|██▋       | 55199/210001 [00:13<00:36, 4238.57files/s]\u001b[A\n",
      " 26%|██▋       | 55622/210001 [00:13<00:36, 4229.72files/s]\u001b[A\n",
      " 27%|██▋       | 56026/210001 [00:13<00:36, 4228.27files/s]\u001b[A\n",
      " 27%|██▋       | 56425/210001 [00:13<00:36, 4221.40files/s]\u001b[A\n",
      " 27%|██▋       | 56807/210001 [00:13<00:36, 4212.60files/s]\u001b[A\n",
      " 27%|██▋       | 57171/210001 [00:13<00:36, 4207.89files/s]\u001b[A\n",
      " 27%|██▋       | 57540/210001 [00:13<00:36, 4204.07files/s]\u001b[A\n",
      " 28%|██▊       | 57962/210001 [00:13<00:36, 4204.09files/s]\u001b[A\n",
      " 28%|██▊       | 58389/210001 [00:13<00:36, 4204.60files/s]\u001b[A\n",
      " 28%|██▊       | 58822/210001 [00:13<00:35, 4205.46files/s]\u001b[A\n",
      " 28%|██▊       | 59229/210001 [00:14<00:35, 4201.26files/s]\u001b[A\n",
      " 28%|██▊       | 59624/210001 [00:14<00:35, 4198.88files/s]\u001b[A\n",
      " 29%|██▊       | 60031/210001 [00:14<00:35, 4197.96files/s]\u001b[A\n",
      " 29%|██▉       | 60428/210001 [00:14<00:35, 4195.72files/s]\u001b[A\n",
      " 29%|██▉       | 60822/210001 [00:14<00:35, 4193.75files/s]\u001b[A\n",
      " 29%|██▉       | 61217/210001 [00:14<00:35, 4192.04files/s]\u001b[A\n",
      " 29%|██▉       | 61611/210001 [00:14<00:35, 4189.17files/s]\u001b[A\n",
      " 30%|██▉       | 62001/210001 [00:14<00:35, 4187.14files/s]\u001b[A\n",
      " 30%|██▉       | 62438/210001 [00:14<00:35, 4188.24files/s]\u001b[A\n",
      " 30%|██▉       | 62842/210001 [00:15<00:35, 4186.85files/s]\u001b[A\n",
      " 30%|███       | 63244/210001 [00:15<00:35, 4183.79files/s]\u001b[A\n",
      " 30%|███       | 63644/210001 [00:15<00:34, 4182.61files/s]\u001b[A\n",
      " 30%|███       | 64043/210001 [00:15<00:34, 4181.28files/s]\u001b[A\n",
      " 31%|███       | 64465/210001 [00:15<00:34, 4181.49files/s]\u001b[A\n",
      " 31%|███       | 64870/210001 [00:15<00:34, 4178.31files/s]\u001b[A\n",
      " 31%|███       | 65265/210001 [00:15<00:34, 4176.79files/s]\u001b[A\n",
      " 31%|███▏      | 65660/210001 [00:15<00:34, 4173.67files/s]\u001b[A\n",
      " 31%|███▏      | 66048/210001 [00:15<00:34, 4170.95files/s]\u001b[A\n",
      " 32%|███▏      | 66432/210001 [00:15<00:34, 4167.56files/s]\u001b[A\n",
      " 32%|███▏      | 66811/210001 [00:16<00:34, 4156.57files/s]\u001b[A\n",
      " 32%|███▏      | 67172/210001 [00:16<00:34, 4153.21files/s]\u001b[A\n",
      " 32%|███▏      | 67549/210001 [00:16<00:34, 4150.82files/s]\u001b[A\n",
      " 32%|███▏      | 67910/210001 [00:16<00:34, 4145.05files/s]\u001b[A\n",
      " 33%|███▎      | 68261/210001 [00:16<00:34, 4139.95files/s]\u001b[A\n",
      " 33%|███▎      | 68675/210001 [00:16<00:34, 4139.97files/s]\u001b[A\n",
      " 33%|███▎      | 69149/210001 [00:16<00:33, 4143.52files/s]\u001b[A\n",
      " 33%|███▎      | 69598/210001 [00:16<00:33, 4145.59files/s]\u001b[A\n",
      " 33%|███▎      | 70043/210001 [00:16<00:33, 4147.39files/s]\u001b[A\n",
      " 34%|███▎      | 70488/210001 [00:16<00:33, 4149.12files/s]\u001b[A\n",
      " 34%|███▍      | 70931/210001 [00:17<00:33, 4150.69files/s]\u001b[A\n",
      " 34%|███▍      | 71394/210001 [00:17<00:33, 4153.45files/s]\u001b[A\n",
      " 34%|███▍      | 71871/210001 [00:17<00:33, 4157.10files/s]\u001b[A\n",
      " 34%|███▍      | 72343/210001 [00:17<00:33, 4160.25files/s]\u001b[A\n",
      " 35%|███▍      | 72802/210001 [00:17<00:32, 4161.49files/s]\u001b[A\n",
      " 35%|███▍      | 73260/210001 [00:17<00:32, 4163.82files/s]\u001b[A\n",
      " 35%|███▌      | 73714/210001 [00:17<00:32, 4165.47files/s]\u001b[A\n",
      " 35%|███▌      | 74165/210001 [00:17<00:32, 4162.79files/s]\u001b[A\n",
      " 36%|███▌      | 74594/210001 [00:17<00:32, 4161.24files/s]\u001b[A\n",
      " 36%|███▌      | 75012/210001 [00:18<00:32, 4160.00files/s]\u001b[A\n",
      " 36%|███▌      | 75423/210001 [00:18<00:32, 4159.26files/s]\u001b[A\n",
      " 36%|███▌      | 75832/210001 [00:18<00:32, 4158.21files/s]\u001b[A\n",
      " 36%|███▋      | 76238/210001 [00:18<00:32, 4155.13files/s]\u001b[A\n",
      " 36%|███▋      | 76643/210001 [00:18<00:32, 4154.54files/s]\u001b[A\n",
      " 37%|███▋      | 77048/210001 [00:18<00:32, 4153.97files/s]\u001b[A\n",
      " 37%|███▋      | 77448/210001 [00:18<00:31, 4150.69files/s]\u001b[A\n",
      " 37%|███▋      | 77845/210001 [00:18<00:31, 4149.71files/s]\u001b[A\n",
      " 37%|███▋      | 78266/210001 [00:18<00:31, 4149.98files/s]\u001b[A\n",
      " 37%|███▋      | 78692/210001 [00:18<00:31, 4150.61files/s]\u001b[A\n",
      " 38%|███▊      | 79140/210001 [00:19<00:31, 4152.30files/s]\u001b[A\n",
      " 38%|███▊      | 79560/210001 [00:19<00:31, 4151.18files/s]\u001b[A\n",
      " 38%|███▊      | 79997/210001 [00:19<00:31, 4152.27files/s]\u001b[A\n",
      " 38%|███▊      | 80475/210001 [00:19<00:31, 4155.46files/s]\u001b[A\n",
      " 39%|███▊      | 80915/210001 [00:19<00:31, 4156.70files/s]\u001b[A\n",
      " 39%|███▉      | 81376/210001 [00:19<00:30, 4159.07files/s]\u001b[A\n",
      " 39%|███▉      | 81864/210001 [00:19<00:30, 4162.66files/s]\u001b[A\n",
      " 39%|███▉      | 82322/210001 [00:19<00:30, 4164.07files/s]\u001b[A\n",
      " 39%|███▉      | 82781/210001 [00:19<00:30, 4166.13files/s]\u001b[A\n",
      " 40%|███▉      | 83245/210001 [00:19<00:30, 4168.50files/s]\u001b[A\n",
      " 40%|███▉      | 83705/210001 [00:20<00:30, 4170.50files/s]\u001b[A\n",
      " 40%|████      | 84163/210001 [00:20<00:30, 4171.78files/s]\u001b[A\n",
      " 40%|████      | 84616/210001 [00:20<00:30, 4172.58files/s]\u001b[A\n",
      " 41%|████      | 85089/210001 [00:20<00:29, 4175.30files/s]\u001b[A\n",
      " 41%|████      | 85544/210001 [00:20<00:29, 4173.66files/s]\u001b[A\n",
      " 41%|████      | 85979/210001 [00:20<00:29, 4173.88files/s]\u001b[A\n",
      " 41%|████      | 86416/210001 [00:20<00:29, 4174.79files/s]\u001b[A\n",
      " 41%|████▏     | 86849/210001 [00:20<00:29, 4174.54files/s]\u001b[A\n",
      " 42%|████▏     | 87276/210001 [00:20<00:29, 4173.42files/s]\u001b[A\n",
      " 42%|████▏     | 87694/210001 [00:21<00:29, 4173.04files/s]\u001b[A\n",
      " 42%|████▏     | 88110/210001 [00:21<00:29, 4170.56files/s]\u001b[A\n",
      " 42%|████▏     | 88512/210001 [00:21<00:29, 4168.98files/s]\u001b[A\n",
      " 42%|████▏     | 88926/210001 [00:21<00:29, 4168.74files/s]\u001b[A\n",
      " 43%|████▎     | 89328/210001 [00:21<00:28, 4167.03files/s]\u001b[A\n",
      " 43%|████▎     | 89724/210001 [00:21<00:28, 4164.45files/s]\u001b[A\n",
      " 43%|████▎     | 90141/210001 [00:21<00:28, 4164.44files/s]\u001b[A\n",
      " 43%|████▎     | 90537/210001 [00:21<00:28, 4161.96files/s]\u001b[A\n",
      " 43%|████▎     | 90977/210001 [00:21<00:28, 4163.02files/s]\u001b[A\n",
      " 44%|████▎     | 91380/210001 [00:21<00:28, 4161.13files/s]\u001b[A\n",
      " 44%|████▎     | 91808/210001 [00:22<00:28, 4161.67files/s]\u001b[A\n",
      " 44%|████▍     | 92281/210001 [00:22<00:28, 4164.17files/s]\u001b[A\n",
      " 44%|████▍     | 92716/210001 [00:22<00:28, 4165.02files/s]\u001b[A\n",
      " 44%|████▍     | 93152/210001 [00:22<00:28, 4165.88files/s]\u001b[A\n",
      " 45%|████▍     | 93583/210001 [00:22<00:27, 4163.74files/s]\u001b[A\n",
      " 45%|████▍     | 94021/210001 [00:22<00:27, 4164.70files/s]\u001b[A\n",
      " 45%|████▍     | 94472/210001 [00:22<00:27, 4166.22files/s]\u001b[A\n",
      " 45%|████▌     | 94940/210001 [00:22<00:27, 4168.45files/s]\u001b[A\n",
      " 45%|████▌     | 95428/210001 [00:22<00:27, 4171.53files/s]\u001b[A\n",
      " 46%|████▌     | 95884/210001 [00:22<00:27, 4173.00files/s]\u001b[A\n",
      " 46%|████▌     | 96338/210001 [00:23<00:27, 4172.51files/s]\u001b[A\n",
      " 46%|████▌     | 96778/210001 [00:23<00:27, 4171.08files/s]\u001b[A\n",
      " 46%|████▋     | 97203/210001 [00:23<00:27, 4171.25files/s]\u001b[A\n",
      " 47%|████▋     | 97666/210001 [00:23<00:26, 4173.16files/s]\u001b[A\n",
      " 47%|████▋     | 98110/210001 [00:23<00:26, 4174.29files/s]\u001b[A\n",
      " 47%|████▋     | 98592/210001 [00:23<00:26, 4177.01files/s]\u001b[A\n",
      " 47%|████▋     | 99043/210001 [00:23<00:26, 4177.05files/s]\u001b[A\n",
      " 47%|████▋     | 99489/210001 [00:23<00:26, 4178.24files/s]\u001b[A\n",
      " 48%|████▊     | 99932/210001 [00:23<00:26, 4176.77files/s]\u001b[A\n",
      " 48%|████▊     | 100358/210001 [00:24<00:26, 4175.98files/s]\u001b[A\n",
      " 48%|████▊     | 100801/210001 [00:24<00:26, 4177.06files/s]\u001b[A\n",
      " 48%|████▊     | 101227/210001 [00:24<00:26, 4176.44files/s]\u001b[A\n",
      " 48%|████▊     | 101647/210001 [00:24<00:25, 4175.39files/s]\u001b[A\n",
      " 49%|████▊     | 102059/210001 [00:24<00:25, 4174.54files/s]\u001b[A\n",
      " 49%|████▉     | 102467/210001 [00:24<00:25, 4173.24files/s]\u001b[A\n",
      " 49%|████▉     | 102909/210001 [00:24<00:25, 4174.23files/s]\u001b[A\n",
      " 49%|████▉     | 103323/210001 [00:24<00:25, 4171.55files/s]\u001b[A\n",
      " 49%|████▉     | 103721/210001 [00:24<00:25, 4167.68files/s]\u001b[A\n",
      " 50%|████▉     | 104172/210001 [00:24<00:25, 4169.06files/s]\u001b[A\n",
      " 50%|████▉     | 104666/210001 [00:25<00:25, 4172.12files/s]\u001b[A\n",
      " 50%|█████     | 105124/210001 [00:25<00:25, 4173.68files/s]\u001b[A\n",
      " 50%|█████     | 105564/210001 [00:25<00:25, 4174.54files/s]\u001b[A\n",
      " 50%|█████     | 106002/210001 [00:25<00:24, 4174.74files/s]\u001b[A\n",
      " 51%|█████     | 106435/210001 [00:25<00:24, 4173.69files/s]\u001b[A\n",
      " 51%|█████     | 106857/210001 [00:25<00:24, 4173.51files/s]\u001b[A\n",
      " 51%|█████     | 107279/210001 [00:25<00:24, 4173.70files/s]\u001b[A\n",
      " 51%|█████▏    | 107705/210001 [00:25<00:24, 4174.03files/s]\u001b[A\n",
      " 51%|█████▏    | 108127/210001 [00:25<00:24, 4173.57files/s]\u001b[A\n",
      " 52%|█████▏    | 108562/210001 [00:26<00:24, 4174.19files/s]\u001b[A\n",
      " 52%|█████▏    | 108984/210001 [00:26<00:24, 4173.75files/s]\u001b[A\n",
      " 52%|█████▏    | 109411/210001 [00:26<00:24, 4174.08files/s]\u001b[A\n",
      " 52%|█████▏    | 109844/210001 [00:26<00:23, 4174.68files/s]\u001b[A\n",
      " 53%|█████▎    | 110268/210001 [00:26<00:23, 4174.86files/s]\u001b[A\n",
      " 53%|█████▎    | 110692/210001 [00:26<00:23, 4172.14files/s]\u001b[A\n",
      " 53%|█████▎    | 111096/210001 [00:26<00:23, 4171.29files/s]\u001b[A\n",
      " 53%|█████▎    | 111497/210001 [00:26<00:23, 4168.14files/s]\u001b[A\n",
      " 53%|█████▎    | 111881/210001 [00:26<00:23, 4164.22files/s]\u001b[A\n",
      " 53%|█████▎    | 112270/210001 [00:26<00:23, 4163.19files/s]\u001b[A\n",
      " 54%|█████▎    | 112644/210001 [00:27<00:23, 4160.10files/s]\u001b[A\n",
      " 54%|█████▍    | 113020/210001 [00:27<00:23, 4158.60files/s]\u001b[A\n",
      " 54%|█████▍    | 113425/210001 [00:27<00:23, 4158.17files/s]\u001b[A\n",
      " 54%|█████▍    | 113860/210001 [00:27<00:23, 4158.88files/s]\u001b[A\n",
      " 54%|█████▍    | 114262/210001 [00:27<00:23, 4158.31files/s]\u001b[A\n",
      " 55%|█████▍    | 114688/210001 [00:27<00:22, 4158.67files/s]\u001b[A\n",
      " 55%|█████▍    | 115116/210001 [00:27<00:22, 4159.09files/s]\u001b[A\n",
      " 55%|█████▌    | 115529/210001 [00:27<00:22, 4158.34files/s]\u001b[A\n",
      " 55%|█████▌    | 115945/210001 [00:27<00:22, 4158.33files/s]\u001b[A\n",
      " 55%|█████▌    | 116361/210001 [00:27<00:22, 4158.34files/s]\u001b[A\n",
      " 56%|█████▌    | 116787/210001 [00:28<00:22, 4158.70files/s]\u001b[A\n",
      " 56%|█████▌    | 117203/210001 [00:28<00:22, 4158.33files/s]\u001b[A\n",
      " 56%|█████▌    | 117640/210001 [00:28<00:22, 4159.04files/s]\u001b[A\n",
      " 56%|█████▌    | 118060/210001 [00:28<00:22, 4158.81files/s]\u001b[A\n",
      " 56%|█████▋    | 118478/210001 [00:28<00:22, 4157.07files/s]\u001b[A\n",
      " 57%|█████▋    | 118882/210001 [00:28<00:21, 4156.13files/s]\u001b[A\n",
      " 57%|█████▋    | 119290/210001 [00:28<00:21, 4155.87files/s]\u001b[A\n",
      " 57%|█████▋    | 119727/210001 [00:28<00:21, 4156.56files/s]\u001b[A\n",
      " 57%|█████▋    | 120150/210001 [00:28<00:21, 4156.72files/s]\u001b[A\n",
      " 57%|█████▋    | 120566/210001 [00:29<00:21, 4155.71files/s]\u001b[A\n",
      " 58%|█████▊    | 120973/210001 [00:29<00:21, 4150.92files/s]\u001b[A\n",
      " 58%|█████▊    | 121351/210001 [00:29<00:21, 4147.88files/s]\u001b[A\n",
      " 58%|█████▊    | 121770/210001 [00:29<00:21, 4148.00files/s]\u001b[A\n",
      " 58%|█████▊    | 122176/210001 [00:29<00:21, 4147.69files/s]\u001b[A\n",
      " 58%|█████▊    | 122577/210001 [00:29<00:21, 4147.19files/s]\u001b[A\n",
      " 59%|█████▊    | 123020/210001 [00:29<00:20, 4148.12files/s]\u001b[A\n",
      " 59%|█████▉    | 123465/210001 [00:29<00:20, 4149.10files/s]\u001b[A\n",
      " 59%|█████▉    | 123911/210001 [00:29<00:20, 4150.11files/s]\u001b[A\n",
      " 59%|█████▉    | 124338/210001 [00:29<00:20, 4150.17files/s]\u001b[A\n",
      " 59%|█████▉    | 124762/210001 [00:30<00:20, 4150.27files/s]\u001b[A\n",
      " 60%|█████▉    | 125237/210001 [00:30<00:20, 4152.23files/s]\u001b[A\n",
      " 60%|█████▉    | 125710/210001 [00:30<00:20, 4154.12files/s]\u001b[A\n",
      " 60%|██████    | 126189/210001 [00:30<00:20, 4156.14files/s]\u001b[A\n",
      " 60%|██████    | 126654/210001 [00:30<00:20, 4157.73files/s]\u001b[A\n",
      " 61%|██████    | 127114/210001 [00:30<00:19, 4158.98files/s]\u001b[A\n",
      " 61%|██████    | 127572/210001 [00:30<00:19, 4159.52files/s]\u001b[A\n",
      " 61%|██████    | 128022/210001 [00:30<00:19, 4160.04files/s]\u001b[A\n",
      " 61%|██████    | 128488/210001 [00:30<00:19, 4161.62files/s]\u001b[A\n",
      " 61%|██████▏   | 128951/210001 [00:30<00:19, 4163.16files/s]\u001b[A\n",
      " 62%|██████▏   | 129444/210001 [00:31<00:19, 4165.61files/s]\u001b[A\n",
      " 62%|██████▏   | 129915/210001 [00:31<00:19, 4167.32files/s]\u001b[A\n",
      " 62%|██████▏   | 130391/210001 [00:31<00:19, 4169.22files/s]\u001b[A\n",
      " 62%|██████▏   | 130861/210001 [00:31<00:18, 4170.15files/s]\u001b[A\n",
      " 63%|██████▎   | 131323/210001 [00:31<00:18, 4170.37files/s]\u001b[A\n",
      " 63%|██████▎   | 131775/210001 [00:31<00:18, 4171.43files/s]\u001b[A\n",
      " 63%|██████▎   | 132226/210001 [00:31<00:18, 4171.62files/s]\u001b[A\n",
      " 63%|██████▎   | 132669/210001 [00:31<00:18, 4170.69files/s]\u001b[A\n",
      " 63%|██████▎   | 133096/210001 [00:31<00:18, 4170.20files/s]\u001b[A\n",
      " 64%|██████▎   | 133516/210001 [00:32<00:18, 4169.72files/s]\u001b[A\n",
      " 64%|██████▍   | 133931/210001 [00:32<00:18, 4168.76files/s]\u001b[A\n",
      " 64%|██████▍   | 134338/210001 [00:32<00:18, 4168.30files/s]\u001b[A\n",
      " 64%|██████▍   | 134743/210001 [00:32<00:18, 4165.43files/s]\u001b[A\n",
      " 64%|██████▍   | 135165/210001 [00:32<00:17, 4165.58files/s]\u001b[A\n",
      " 65%|██████▍   | 135599/210001 [00:32<00:17, 4166.11files/s]\u001b[A\n",
      " 65%|██████▍   | 136047/210001 [00:32<00:17, 4166.96files/s]\u001b[A\n",
      " 65%|██████▍   | 136477/210001 [00:32<00:17, 4167.34files/s]\u001b[A\n",
      " 65%|██████▌   | 136931/210001 [00:32<00:17, 4168.47files/s]\u001b[A\n",
      " 65%|██████▌   | 137363/210001 [00:32<00:17, 4168.84files/s]\u001b[A\n",
      " 66%|██████▌   | 137799/210001 [00:33<00:17, 4169.41files/s]\u001b[A\n",
      " 66%|██████▌   | 138254/210001 [00:33<00:17, 4170.54files/s]\u001b[A\n",
      " 66%|██████▌   | 138693/210001 [00:33<00:17, 4170.67files/s]\u001b[A\n",
      " 66%|██████▋   | 139127/210001 [00:33<00:16, 4169.63files/s]\u001b[A\n",
      " 66%|██████▋   | 139547/210001 [00:33<00:16, 4168.92files/s]\u001b[A\n",
      " 67%|██████▋   | 139979/210001 [00:33<00:16, 4169.35files/s]\u001b[A\n",
      " 67%|██████▋   | 140397/210001 [00:33<00:16, 4168.73files/s]\u001b[A\n",
      " 67%|██████▋   | 140810/210001 [00:33<00:16, 4167.76files/s]\u001b[A\n",
      " 67%|██████▋   | 141214/210001 [00:33<00:16, 4167.31files/s]\u001b[A\n",
      " 67%|██████▋   | 141650/210001 [00:33<00:16, 4167.87files/s]\u001b[A\n",
      " 68%|██████▊   | 142091/210001 [00:34<00:16, 4168.55files/s]\u001b[A\n",
      " 68%|██████▊   | 142550/210001 [00:34<00:16, 4169.78files/s]\u001b[A\n",
      " 68%|██████▊   | 142983/210001 [00:34<00:16, 4170.23files/s]\u001b[A\n",
      " 68%|██████▊   | 143416/210001 [00:34<00:15, 4166.31files/s]\u001b[A\n",
      " 68%|██████▊   | 143815/210001 [00:34<00:15, 4162.72files/s]\u001b[A\n",
      " 69%|██████▊   | 144214/210001 [00:34<00:15, 4162.21files/s]\u001b[A\n",
      " 69%|██████▉   | 144642/210001 [00:34<00:15, 4162.56files/s]\u001b[A\n",
      " 69%|██████▉   | 145104/210001 [00:34<00:15, 4163.84files/s]\u001b[A\n",
      " 69%|██████▉   | 145561/210001 [00:34<00:15, 4165.02files/s]\u001b[A\n",
      " 70%|██████▉   | 146032/210001 [00:35<00:15, 4166.57files/s]\u001b[A\n",
      " 70%|██████▉   | 146475/210001 [00:35<00:15, 4167.30files/s]\u001b[A\n",
      " 70%|██████▉   | 146917/210001 [00:35<00:15, 4167.57files/s]\u001b[A\n",
      " 70%|███████   | 147354/210001 [00:35<00:15, 4167.93files/s]\u001b[A\n",
      " 70%|███████   | 147805/210001 [00:35<00:14, 4168.89files/s]\u001b[A\n",
      " 71%|███████   | 148245/210001 [00:35<00:14, 4168.87files/s]\u001b[A\n",
      " 71%|███████   | 148686/210001 [00:35<00:14, 4169.52files/s]\u001b[A\n",
      " 71%|███████   | 149130/210001 [00:35<00:14, 4170.28files/s]\u001b[A\n",
      " 71%|███████   | 149585/210001 [00:35<00:14, 4171.31files/s]\u001b[A\n",
      " 71%|███████▏  | 150043/210001 [00:35<00:14, 4172.46files/s]\u001b[A\n",
      " 72%|███████▏  | 150525/210001 [00:36<00:14, 4174.23files/s]\u001b[A\n",
      " 72%|███████▏  | 150983/210001 [00:36<00:14, 4175.06files/s]\u001b[A\n",
      " 72%|███████▏  | 151463/210001 [00:36<00:14, 4176.76files/s]\u001b[A\n",
      " 72%|███████▏  | 151925/210001 [00:36<00:13, 4177.63files/s]\u001b[A\n",
      " 73%|███████▎  | 152383/210001 [00:36<00:13, 4177.58files/s]\u001b[A\n",
      " 73%|███████▎  | 152829/210001 [00:36<00:13, 4178.23files/s]\u001b[A\n",
      " 73%|███████▎  | 153274/210001 [00:36<00:13, 4178.40files/s]\u001b[A\n",
      " 73%|███████▎  | 153712/210001 [00:36<00:13, 4178.90files/s]\u001b[A\n",
      " 73%|███████▎  | 154150/210001 [00:36<00:13, 4179.40files/s]\u001b[A\n",
      " 74%|███████▎  | 154588/210001 [00:36<00:13, 4179.58files/s]\u001b[A\n",
      " 74%|███████▍  | 155056/210001 [00:37<00:13, 4180.92files/s]\u001b[A\n",
      " 74%|███████▍  | 155504/210001 [00:37<00:13, 4181.70files/s]\u001b[A\n",
      " 74%|███████▍  | 155957/210001 [00:37<00:12, 4182.63files/s]\u001b[A\n",
      " 74%|███████▍  | 156418/210001 [00:37<00:12, 4183.76files/s]\u001b[A\n",
      " 75%|███████▍  | 156870/210001 [00:37<00:12, 4182.05files/s]\u001b[A\n",
      " 75%|███████▍  | 157296/210001 [00:37<00:12, 4182.14files/s]\u001b[A\n",
      " 75%|███████▌  | 157741/210001 [00:37<00:12, 4182.83files/s]\u001b[A\n",
      " 75%|███████▌  | 158172/210001 [00:37<00:12, 4182.77files/s]\u001b[A\n",
      " 76%|███████▌  | 158625/210001 [00:37<00:12, 4183.66files/s]\u001b[A\n",
      " 76%|███████▌  | 159063/210001 [00:38<00:12, 4184.17files/s]\u001b[A\n",
      " 76%|███████▌  | 159511/210001 [00:38<00:12, 4184.95files/s]\u001b[A\n",
      " 76%|███████▌  | 159950/210001 [00:38<00:11, 4185.24files/s]\u001b[A\n",
      " 76%|███████▋  | 160426/210001 [00:38<00:11, 4186.74files/s]\u001b[A\n",
      " 77%|███████▋  | 160926/210001 [00:38<00:11, 4188.83files/s]\u001b[A\n",
      " 77%|███████▋  | 161399/210001 [00:38<00:11, 4190.23files/s]\u001b[A\n",
      " 77%|███████▋  | 161870/210001 [00:38<00:11, 4191.60files/s]\u001b[A\n",
      " 77%|███████▋  | 162338/210001 [00:38<00:11, 4192.37files/s]\u001b[A\n",
      " 78%|███████▊  | 162803/210001 [00:38<00:11, 4193.56files/s]\u001b[A\n",
      " 78%|███████▊  | 163266/210001 [00:38<00:11, 4194.47files/s]\u001b[A\n",
      " 78%|███████▊  | 163728/210001 [00:39<00:11, 4195.51files/s]\u001b[A\n",
      " 78%|███████▊  | 164189/210001 [00:39<00:10, 4196.04files/s]\u001b[A\n",
      " 78%|███████▊  | 164657/210001 [00:39<00:10, 4197.27files/s]\u001b[A\n",
      " 79%|███████▊  | 165116/210001 [00:39<00:10, 4198.11files/s]\u001b[A\n",
      " 79%|███████▉  | 165605/210001 [00:39<00:10, 4199.86files/s]\u001b[A\n",
      " 79%|███████▉  | 166072/210001 [00:39<00:10, 4200.81files/s]\u001b[A\n",
      " 79%|███████▉  | 166536/210001 [00:39<00:10, 4201.44files/s]\u001b[A\n",
      " 80%|███████▉  | 167000/210001 [00:39<00:10, 4202.53files/s]\u001b[A\n",
      " 80%|███████▉  | 167495/210001 [00:39<00:10, 4204.37files/s]\u001b[A\n",
      " 80%|███████▉  | 167965/210001 [00:39<00:09, 4205.36files/s]\u001b[A\n",
      " 80%|████████  | 168432/210001 [00:40<00:09, 4206.09files/s]\u001b[A\n",
      " 80%|████████  | 168893/210001 [00:40<00:09, 4206.85files/s]\u001b[A\n",
      " 81%|████████  | 169384/210001 [00:40<00:09, 4208.59files/s]\u001b[A\n",
      " 81%|████████  | 169863/210001 [00:40<00:09, 4209.98files/s]\u001b[A\n",
      " 81%|████████  | 170367/210001 [00:40<00:09, 4212.03files/s]\u001b[A\n",
      " 81%|████████▏ | 170848/210001 [00:40<00:09, 4211.93files/s]\u001b[A\n",
      " 82%|████████▏ | 171321/210001 [00:40<00:09, 4213.19files/s]\u001b[A\n",
      " 82%|████████▏ | 171816/210001 [00:40<00:09, 4214.96files/s]\u001b[A\n",
      " 82%|████████▏ | 172290/210001 [00:40<00:08, 4216.16files/s]\u001b[A\n",
      " 82%|████████▏ | 172770/210001 [00:40<00:08, 4217.55files/s]\u001b[A\n",
      " 83%|████████▎ | 173257/210001 [00:41<00:08, 4219.11files/s]\u001b[A\n",
      " 83%|████████▎ | 173775/210001 [00:41<00:08, 4221.43files/s]\u001b[A\n",
      " 83%|████████▎ | 174265/210001 [00:41<00:08, 4222.92files/s]\u001b[A\n",
      " 83%|████████▎ | 174762/210001 [00:41<00:08, 4224.73files/s]\u001b[A\n",
      " 83%|████████▎ | 175253/210001 [00:41<00:08, 4225.53files/s]\u001b[A\n",
      " 84%|████████▎ | 175732/210001 [00:41<00:08, 4226.32files/s]\u001b[A\n",
      " 84%|████████▍ | 176204/210001 [00:41<00:07, 4227.20files/s]\u001b[A\n",
      " 84%|████████▍ | 176672/210001 [00:41<00:07, 4228.01files/s]\u001b[A\n",
      " 84%|████████▍ | 177137/210001 [00:41<00:07, 4228.50files/s]\u001b[A\n",
      " 85%|████████▍ | 177595/210001 [00:42<00:07, 4228.04files/s]\u001b[A\n",
      " 85%|████████▍ | 178060/210001 [00:42<00:07, 4229.01files/s]\u001b[A\n",
      " 85%|████████▌ | 178515/210001 [00:42<00:07, 4229.77files/s]\u001b[A\n",
      " 85%|████████▌ | 178966/210001 [00:42<00:07, 4228.44files/s]\u001b[A\n",
      " 85%|████████▌ | 179395/210001 [00:42<00:07, 4227.48files/s]\u001b[A\n",
      " 86%|████████▌ | 179811/210001 [00:42<00:07, 4226.06files/s]\u001b[A\n",
      " 86%|████████▌ | 180230/210001 [00:42<00:07, 4225.97files/s]\u001b[A\n",
      " 86%|████████▌ | 180685/210001 [00:42<00:06, 4226.70files/s]\u001b[A\n",
      " 86%|████████▌ | 181114/210001 [00:42<00:06, 4226.82files/s]\u001b[A\n",
      " 86%|████████▋ | 181558/210001 [00:42<00:06, 4227.27files/s]\u001b[A\n",
      " 87%|████████▋ | 182010/210001 [00:43<00:06, 4227.96files/s]\u001b[A\n",
      " 87%|████████▋ | 182446/210001 [00:43<00:06, 4227.08files/s]\u001b[A\n",
      " 87%|████████▋ | 182868/210001 [00:43<00:06, 4226.85files/s]\u001b[A\n",
      " 87%|████████▋ | 183287/210001 [00:43<00:06, 4226.38files/s]\u001b[A\n",
      " 87%|████████▋ | 183702/210001 [00:43<00:06, 4226.11files/s]\u001b[A\n",
      " 88%|████████▊ | 184116/210001 [00:43<00:06, 4225.54files/s]\u001b[A\n",
      " 88%|████████▊ | 184599/210001 [00:43<00:06, 4226.91files/s]\u001b[A\n",
      " 88%|████████▊ | 185071/210001 [00:43<00:05, 4227.99files/s]\u001b[A\n",
      " 88%|████████▊ | 185569/210001 [00:43<00:05, 4229.74files/s]\u001b[A\n",
      " 89%|████████▊ | 186029/210001 [00:43<00:05, 4230.32files/s]\u001b[A\n",
      " 89%|████████▉ | 186485/210001 [00:44<00:05, 4229.02files/s]\u001b[A\n",
      " 89%|████████▉ | 186917/210001 [00:44<00:05, 4225.90files/s]\u001b[A\n",
      " 89%|████████▉ | 187318/210001 [00:44<00:05, 4222.38files/s]\u001b[A\n",
      " 89%|████████▉ | 187702/210001 [00:44<00:05, 4221.49files/s]\u001b[A\n",
      " 90%|████████▉ | 188113/210001 [00:44<00:05, 4221.25files/s]\u001b[A\n",
      " 90%|████████▉ | 188508/210001 [00:44<00:05, 4220.62files/s]\u001b[A\n",
      " 90%|████████▉ | 188941/210001 [00:44<00:04, 4220.85files/s]\u001b[A\n",
      " 90%|█████████ | 189356/210001 [00:44<00:04, 4220.64files/s]\u001b[A\n",
      " 90%|█████████ | 189761/210001 [00:44<00:04, 4219.74files/s]\u001b[A\n",
      " 91%|█████████ | 190194/210001 [00:45<00:04, 4219.96files/s]\u001b[A\n",
      " 91%|█████████ | 190603/210001 [00:45<00:04, 4219.17files/s]\u001b[A\n",
      " 91%|█████████ | 191006/210001 [00:45<00:04, 4218.65files/s]\u001b[A\n",
      " 91%|█████████ | 191408/210001 [00:45<00:04, 4217.54files/s]\u001b[A\n",
      " 91%|█████████▏| 191802/210001 [00:45<00:04, 4215.46files/s]\u001b[A\n",
      " 92%|█████████▏| 192180/210001 [00:45<00:04, 4214.30files/s]\u001b[A\n",
      " 92%|█████████▏| 192583/210001 [00:45<00:04, 4213.90files/s]\u001b[A\n",
      " 92%|█████████▏| 192993/210001 [00:45<00:04, 4213.38files/s]\u001b[A\n",
      " 92%|█████████▏| 193391/210001 [00:45<00:03, 4212.93files/s]\u001b[A\n",
      " 92%|█████████▏| 193805/210001 [00:46<00:03, 4212.75files/s]\u001b[A\n",
      " 92%|█████████▏| 194204/210001 [00:46<00:03, 4211.76files/s]\u001b[A\n",
      " 93%|█████████▎| 194596/210001 [00:46<00:03, 4211.01files/s]\u001b[A\n",
      " 93%|█████████▎| 195006/210001 [00:46<00:03, 4210.76files/s]\u001b[A\n",
      " 93%|█████████▎| 195447/210001 [00:46<00:03, 4211.15files/s]\u001b[A\n",
      " 93%|█████████▎| 195857/210001 [00:46<00:03, 4210.67files/s]\u001b[A\n",
      " 93%|█████████▎| 196264/210001 [00:46<00:03, 4208.38files/s]\u001b[A\n",
      " 94%|█████████▎| 196682/210001 [00:46<00:03, 4208.33files/s]\u001b[A\n",
      " 94%|█████████▍| 197153/210001 [00:46<00:03, 4209.39files/s]\u001b[A\n",
      " 94%|█████████▍| 197575/210001 [00:46<00:02, 4209.38files/s]\u001b[A\n",
      " 94%|█████████▍| 197994/210001 [00:47<00:02, 4208.65files/s]\u001b[A\n",
      " 94%|█████████▍| 198404/210001 [00:47<00:02, 4207.38files/s]\u001b[A\n",
      " 95%|█████████▍| 198801/210001 [00:47<00:02, 4206.36files/s]\u001b[A\n",
      " 95%|█████████▍| 199214/210001 [00:47<00:02, 4206.18files/s]\u001b[A\n",
      " 95%|█████████▌| 199611/210001 [00:47<00:02, 4205.53files/s]\u001b[A\n",
      " 95%|█████████▌| 200032/210001 [00:47<00:02, 4205.55files/s]\u001b[A\n",
      " 95%|█████████▌| 200468/210001 [00:47<00:02, 4205.83files/s]\u001b[A\n",
      " 96%|█████████▌| 200881/210001 [00:47<00:02, 4205.34files/s]\u001b[A\n",
      " 96%|█████████▌| 201290/210001 [00:47<00:02, 4205.08files/s]\u001b[A\n",
      " 96%|█████████▌| 201762/210001 [00:47<00:01, 4206.13files/s]\u001b[A\n",
      " 96%|█████████▋| 202190/210001 [00:48<00:01, 4203.18files/s]\u001b[A\n",
      " 96%|█████████▋| 202584/210001 [00:48<00:01, 4201.34files/s]\u001b[A\n",
      " 97%|█████████▋| 202963/210001 [00:48<00:01, 4199.40files/s]\u001b[A\n",
      " 97%|█████████▋| 203330/210001 [00:48<00:01, 4197.88files/s]\u001b[A\n",
      " 97%|█████████▋| 203691/210001 [00:48<00:01, 4194.67files/s]\u001b[A\n",
      " 97%|█████████▋| 204032/210001 [00:48<00:01, 4191.70files/s]\u001b[A\n",
      " 97%|█████████▋| 204413/210001 [00:48<00:01, 4190.92files/s]\u001b[A\n",
      " 98%|█████████▊| 204792/210001 [00:48<00:01, 4190.09files/s]\u001b[A\n",
      " 98%|█████████▊| 205197/210001 [00:48<00:01, 4189.79files/s]\u001b[A\n",
      " 98%|█████████▊| 205615/210001 [00:49<00:01, 4189.75files/s]\u001b[A\n",
      " 98%|█████████▊| 205999/210001 [00:49<00:00, 4188.93files/s]\u001b[A\n",
      " 98%|█████████▊| 206382/210001 [00:49<00:00, 4187.89files/s]\u001b[A\n",
      " 98%|█████████▊| 206761/210001 [00:49<00:00, 4186.16files/s]\u001b[A\n",
      " 99%|█████████▊| 207129/210001 [00:49<00:00, 4184.63files/s]\u001b[A\n",
      " 99%|█████████▉| 207491/210001 [00:49<00:00, 4183.46files/s]\u001b[A\n",
      " 99%|█████████▉| 207883/210001 [00:49<00:00, 4182.93files/s]\u001b[A\n",
      " 99%|█████████▉| 208274/210001 [00:49<00:00, 4182.37files/s]\u001b[A\n",
      " 99%|█████████▉| 208651/210001 [00:49<00:00, 4181.11files/s]\u001b[A\n",
      "100%|█████████▉| 209045/210001 [00:50<00:00, 4180.64files/s]\u001b[A\n",
      "100%|█████████▉| 209423/210001 [00:50<00:00, 4179.39files/s]\u001b[A\n",
      "100%|█████████▉| 209796/210001 [00:50<00:00, 4178.43files/s]\u001b[A\n",
      "100%|██████████| 210001/210001 [00:50<00:00, 4176.08files/s]\u001b[A\n",
      "  0%|          | 0/10001 [00:00<?, ?files/s]\u001b[A\n",
      "  4%|▎         | 355/10001 [00:00<00:02, 3516.28files/s]\u001b[A\n",
      "  8%|▊         | 773/10001 [00:00<00:02, 3848.20files/s]\u001b[A\n",
      " 12%|█▏        | 1193/10001 [00:00<00:02, 3954.50files/s]\u001b[A\n",
      " 15%|█▌        | 1545/10001 [00:00<00:02, 3848.99files/s]\u001b[A\n",
      " 18%|█▊        | 1850/10001 [00:00<00:02, 3678.07files/s]\u001b[A\n",
      " 21%|██▏       | 2145/10001 [00:00<00:02, 2693.82files/s]\u001b[A\n",
      " 25%|██▍       | 2499/10001 [00:00<00:02, 2788.66files/s]\u001b[A\n",
      " 29%|██▊       | 2867/10001 [00:00<00:02, 2876.93files/s]\u001b[A\n",
      " 33%|███▎      | 3324/10001 [00:01<00:02, 3032.21files/s]\u001b[A\n",
      " 38%|███▊      | 3794/10001 [00:01<00:01, 3171.41files/s]\u001b[A\n",
      " 43%|████▎     | 4284/10001 [00:01<00:01, 3304.03files/s]\u001b[A\n",
      " 47%|████▋     | 4736/10001 [00:01<00:01, 3390.42files/s]\u001b[A\n",
      " 52%|█████▏    | 5214/10001 [00:01<00:01, 3482.76files/s]\u001b[A\n",
      " 57%|█████▋    | 5690/10001 [00:01<00:01, 3562.82files/s]\u001b[A\n",
      " 62%|██████▏   | 6173/10001 [00:01<00:01, 3636.93files/s]\u001b[A\n",
      " 66%|██████▋   | 6634/10001 [00:01<00:00, 3672.11files/s]\u001b[A\n",
      " 71%|███████   | 7084/10001 [00:01<00:00, 3685.92files/s]\u001b[A\n",
      " 75%|███████▌  | 7516/10001 [00:02<00:00, 3689.40files/s]\u001b[A\n",
      " 79%|███████▉  | 7931/10001 [00:02<00:00, 3701.53files/s]\u001b[A\n",
      " 84%|████████▎ | 8368/10001 [00:02<00:00, 3731.36files/s]\u001b[A\n",
      " 88%|████████▊ | 8803/10001 [00:02<00:00, 3757.79files/s]\u001b[A\n",
      " 92%|█████████▏| 9237/10001 [00:02<00:00, 3781.52files/s]\u001b[A\n",
      " 97%|█████████▋| 9673/10001 [00:02<00:00, 3804.21files/s]\u001b[A\n",
      "100%|██████████| 10001/10001 [00:02<00:00, 3802.90files/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features and labels uncompressed.\n"
     ]
    }
   ],
   "source": [
    "def uncompress_features_labels(file):\n",
    "    \"\"\"\n",
    "    Uncompress features and labels from a zip file\n",
    "    :param file: The zip file to extract the data from\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    with ZipFile(file) as zipf:\n",
    "        # Progress Bar\n",
    "        filenames_pbar = tqdm(zipf.namelist(), unit='files')\n",
    "        \n",
    "        # Get features and labels from all files\n",
    "        for filename in filenames_pbar:\n",
    "            # Check if the file is a directory\n",
    "            if not filename.endswith('/'):\n",
    "                with zipf.open(filename) as image_file:\n",
    "                    image = Image.open(image_file)\n",
    "                    image.load()\n",
    "                    # Load image data as 1 dimensional array\n",
    "                    # We're using float32 to save on memory space\n",
    "                    feature = np.array(image, dtype=np.float32).flatten()\n",
    "\n",
    "                # Get the the letter from the filename.  This is the letter of the image.\n",
    "                label = os.path.split(filename)[1][0]\n",
    "\n",
    "                features.append(feature)\n",
    "                labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Get the features and labels from the zip files\n",
    "train_features, train_labels = uncompress_features_labels('notMNIST_train.zip')\n",
    "test_features, test_labels = uncompress_features_labels('notMNIST_test.zip')\n",
    "\n",
    "# Limit the amount of data to work with a docker container\n",
    "docker_size_limit = 150000\n",
    "train_features, train_labels = resample(train_features, train_labels, n_samples=docker_size_limit)\n",
    "\n",
    "# Set flags for feature engineering.  This will prevent you from skipping an important step.\n",
    "is_features_normal = False\n",
    "is_labels_encod = False\n",
    "\n",
    "# Wait until you see that all features and labels have been uncompressed.\n",
    "print('All features and labels uncompressed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/Mean_Variance_Image.png\" style=\"height: 75%;width: 75%; position: relative; right: 5%\">\n",
    "## Problem 1\n",
    "The first problem involves normalizing the features for your training and test data.\n",
    "\n",
    "Implement Min-Max scaling in the `normalize_grayscale()` function to a range of `a=0.1` and `b=0.9`. After scaling, the values of the pixels in the input data should range from 0.1 to 0.9.\n",
    "\n",
    "Since the raw notMNIST image data is in [grayscale](https://en.wikipedia.org/wiki/Grayscale), the current values range from a min of 0 to a max of 255.\n",
    "\n",
    "Min-Max Scaling:\n",
    "$\n",
    "X'=a+{\\frac {\\left(X-X_{\\min }\\right)\\left(b-a\\right)}{X_{\\max }-X_{\\min }}}\n",
    "$\n",
    "\n",
    "*If you're having trouble solving problem 1, you can view the solution [here](https://github.com/udacity/deep-learning/blob/master/intro-to-tensorflow/intro_to_tensorflow_solution.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,  11.,\n",
       "       202., 244., 236., 239., 241., 229., 207., 200., 195., 200., 205.,\n",
       "       225., 241., 239., 237., 243., 174.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   1.,  25.,  46.,  63.,  83.,  94.,\n",
       "       145., 255., 253., 255., 252., 255., 173.,  93.,  85.,  63.,  44.,\n",
       "        21.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   2.,   0.,  49., 255., 250., 253., 248.,\n",
       "       255.,  99.,   0.,   5.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   2.,   3.,   6.,\n",
       "         0.,  52., 255., 252., 255., 250., 255., 102.,   0.,   8.,   3.,\n",
       "         2.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   2.,   0.,  49., 255., 252., 255.,\n",
       "       250., 255., 100.,   0.,   5.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         2.,   0.,  49., 255., 252., 255., 250., 255., 100.,   0.,   5.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   2.,   0.,  49., 255., 252.,\n",
       "       255., 250., 255., 100.,   0.,   5.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   2.,   0.,  49., 255., 252., 255., 250., 255., 100.,   0.,\n",
       "         5.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   2.,   0.,  49., 255.,\n",
       "       252., 255., 250., 255., 100.,   0.,   5.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   2.,   0.,  49., 255., 252., 255., 250., 255., 100.,\n",
       "         0.,   5.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   2.,   0.,  49.,\n",
       "       255., 252., 255., 250., 255., 100.,   0.,   5.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   2.,   0.,  50., 255., 252., 255., 250., 255.,\n",
       "        99.,   0.,   5.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   2.,   0.,\n",
       "        57., 255., 252., 255., 251., 255.,  92.,   0.,   5.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   2.,   0.,  63., 255., 252., 255., 252.,\n",
       "       255.,  84.,   0.,   4.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   2.,\n",
       "         0.,  69., 255., 252., 254., 254., 255.,  77.,   0.,   4.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   2.,   0.,  76., 255., 252., 254.,\n",
       "       255., 251.,  66.,   0.,   4.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         2.,   0.,  81., 255., 252., 253., 255., 230.,  40.,   0.,   2.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   2.,   0.,  94., 255., 252.,\n",
       "       252., 255., 207.,  11.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   2.,   0., 123., 255., 253., 251., 255., 184.,   0.,   4.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0., 153., 255.,\n",
       "       253., 253., 255., 130.,   0.,   3.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   1.,   0., 183., 255., 253., 255., 250.,  46.,   0.,\n",
       "         1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0., 213.,\n",
       "       255., 251., 255., 184.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   2.,   3.,   2.,   1.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   1.,   0.,  31., 238., 255., 252., 254.,  64.,   0.,\n",
       "         3.,   0.,   0.,   0.,   0.,   0.,   1.,   3.,   0.,   0.,   0.,\n",
       "         0.,   2.,   4.,   1.,   0.,   0.,   0.,   0.,   4.,   0., 101.,\n",
       "       255., 250., 255., 136.,   0.,   2.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   2.,   0.,  34.,  64.,  46.,  10.,   0.,   0.,   0.,   3.,\n",
       "         4.,   4.,   5.,   3.,   0., 181., 255., 255., 160.,   1.,   1.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,  11., 140., 247., 255.,\n",
       "       253., 226., 159.,  72.,   8.,   0.,   1.,   0.,   2.,   0., 114.,\n",
       "       255., 246., 123.,   2.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0., 163., 255., 253., 255., 255., 255., 255., 255., 225.,\n",
       "       164., 124., 112., 116., 157., 227., 150.,  38.,   0.,   3.,   1.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  86., 127., 171.,\n",
       "       206., 229., 243., 249., 250., 254., 251., 232., 203., 157.,  94.,\n",
       "        19.,   0.,   1.,   3.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed!\n"
     ]
    }
   ],
   "source": [
    "# Problem 1 - Implement Min-Max scaling for grayscale image data\n",
    "def normalize_grayscale(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Min-Max scaling for grayscale image data\n",
    "    \n",
    "    a = 0.1\n",
    "    b = 0.9\n",
    "    xmin = 0\n",
    "    xmax = 255\n",
    "    return a + ( (image_data-xmin) * (b-a)/(xmax-xmin) )\n",
    "\n",
    "\n",
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "# Test Cases\n",
    "np.testing.assert_array_almost_equal(\n",
    "    normalize_grayscale(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 255])),\n",
    "    [0.1, 0.103137254902, 0.106274509804, 0.109411764706, 0.112549019608, 0.11568627451, 0.118823529412, 0.121960784314,\n",
    "     0.125098039216, 0.128235294118, 0.13137254902, 0.9],\n",
    "    decimal=3)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    normalize_grayscale(np.array([0, 1, 10, 20, 30, 40, 233, 244, 254,255])),\n",
    "    [0.1, 0.103137254902, 0.13137254902, 0.162745098039, 0.194117647059, 0.225490196078, 0.830980392157, 0.865490196078,\n",
    "     0.896862745098, 0.9])\n",
    "\n",
    "if not is_features_normal:\n",
    "    train_features = normalize_grayscale(train_features)\n",
    "    test_features = normalize_grayscale(test_features)\n",
    "    is_features_normal = True\n",
    "\n",
    "print('Tests Passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels One-Hot Encoded\n"
     ]
    }
   ],
   "source": [
    "if not is_labels_encod:\n",
    "    # Turn labels into numbers and apply One-Hot Encoding\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(train_labels)\n",
    "    train_labels = encoder.transform(train_labels)\n",
    "    test_labels = encoder.transform(test_labels)\n",
    "\n",
    "    # Change to float32, so it can be multiplied against the features in TensorFlow, which are float32\n",
    "    train_labels = train_labels.astype(np.float32)\n",
    "    test_labels = test_labels.astype(np.float32)\n",
    "    is_labels_encod = True\n",
    "\n",
    "print('Labels One-Hot Encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features and labels randomized and split.\n"
     ]
    }
   ],
   "source": [
    "assert is_features_normal, 'You skipped the step to normalize the features'\n",
    "assert is_labels_encod, 'You skipped the step to One-Hot Encode the labels'\n",
    "\n",
    "# Get randomized datasets for training and validation\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    test_size=0.05,\n",
    "    random_state=832289)\n",
    "\n",
    "print('Training features and labels randomized and split.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cached in pickle file.\n"
     ]
    }
   ],
   "source": [
    "# Save the data for easy access\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "if not os.path.isfile(pickle_file):\n",
    "    print('Saving data to pickle file...')\n",
    "    try:\n",
    "        with open('notMNIST.pickle', 'wb') as pfile:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    'train_dataset': train_features,\n",
    "                    'train_labels': train_labels,\n",
    "                    'valid_dataset': valid_features,\n",
    "                    'valid_labels': valid_labels,\n",
    "                    'test_dataset': test_features,\n",
    "                    'test_labels': test_labels,\n",
    "                },\n",
    "                pfile, pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "\n",
    "print('Data cached in pickle file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "All your progress is now saved to the pickle file.  If you need to leave and comeback to this lab, you no longer have to start from the beginning.  Just run the code block below and it will load all the data and modules required to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.5.2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python3/3.5.2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python3/3.5.2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data and modules loaded.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Load the modules\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reload the data\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  pickle_data = pickle.load(f)\n",
    "  train_features = pickle_data['train_dataset']\n",
    "  train_labels = pickle_data['train_labels']\n",
    "  valid_features = pickle_data['valid_dataset']\n",
    "  valid_labels = pickle_data['valid_labels']\n",
    "  test_features = pickle_data['test_dataset']\n",
    "  test_labels = pickle_data['test_labels']\n",
    "  del pickle_data  # Free up memory\n",
    "\n",
    "print('Data and modules loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 2\n",
    "\n",
    "Now it's time to build a simple neural network using TensorFlow. Here, your network will be just an input layer and an output layer.\n",
    "\n",
    "<img src=\"image/network_diagram.png\" style=\"height: 40%;width: 40%; position: relative; right: 10%\">\n",
    "\n",
    "For the input here the images have been flattened into a vector of $28 \\times 28 = 784$ features. Then, we're trying to predict the image digit so there are 10 output units, one for each label. Of course, feel free to add hidden layers if you want, but this notebook is built to guide you through a single layer network. \n",
    "\n",
    "For the neural network to train on your data, you need the following <a href=\"https://www.tensorflow.org/resources/dims_types.html#data-types\">float32</a> tensors:\n",
    " - `features`\n",
    "  - Placeholder tensor for feature data (`train_features`/`valid_features`/`test_features`)\n",
    " - `labels`\n",
    "  - Placeholder tensor for label data (`train_labels`/`valid_labels`/`test_labels`)\n",
    " - `weights`\n",
    "  - Variable Tensor with random numbers from a truncated normal distribution.\n",
    "    - See <a href=\"https://www.tensorflow.org/api_docs/python/constant_op.html#truncated_normal\">`tf.truncated_normal()` documentation</a> for help.\n",
    " - `biases`\n",
    "  - Variable Tensor with all zeros.\n",
    "    - See <a href=\"https://www.tensorflow.org/api_docs/python/constant_op.html#zeros\"> `tf.zeros()` documentation</a> for help.\n",
    "\n",
    "*If you're having trouble solving problem 2, review \"TensorFlow Linear Function\" section of the class.  If that doesn't help, the solution for this problem is available [here](intro_to_tensorflow_solution.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed!\n"
     ]
    }
   ],
   "source": [
    "# All the pixels in the image (28 * 28 = 784)\n",
    "features_count = 784\n",
    "# All the labels\n",
    "labels_count = 10\n",
    "\n",
    "# TODO: Set the features and labels tensors\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32) \n",
    "\n",
    "# TODO: Set the weights and biases tensors\n",
    "weights = tf.Variable( tf.truncated_normal((features_count, labels_count)) )\n",
    "biases = tf.Variable(tf.zeros(labels_count))\n",
    "\n",
    "\n",
    "\n",
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "\n",
    "#Test Cases\n",
    "from tensorflow.python.ops.variables import Variable\n",
    "\n",
    "assert features._op.name.startswith('Placeholder'), 'features must be a placeholder'\n",
    "assert labels._op.name.startswith('Placeholder'), 'labels must be a placeholder'\n",
    "assert isinstance(weights, Variable), 'weights must be a TensorFlow variable'\n",
    "assert isinstance(biases, Variable), 'biases must be a TensorFlow variable'\n",
    "\n",
    "assert features._shape == None or (\\\n",
    "    features._shape.dims[0].value is None and\\\n",
    "    features._shape.dims[1].value in [None, 784]), 'The shape of features is incorrect'\n",
    "assert labels._shape  == None or (\\\n",
    "    labels._shape.dims[0].value is None and\\\n",
    "    labels._shape.dims[1].value in [None, 10]), 'The shape of labels is incorrect'\n",
    "assert weights._variable._shape == (784, 10), 'The shape of weights is incorrect'\n",
    "assert biases._variable._shape == (10), 'The shape of biases is incorrect'\n",
    "\n",
    "assert features._dtype == tf.float32, 'features must be type float32'\n",
    "assert labels._dtype == tf.float32, 'labels must be type float32'\n",
    "\n",
    "# Feed dicts for training, validation, and test session\n",
    "train_feed_dict = {features: train_features, labels: train_labels}\n",
    "valid_feed_dict = {features: valid_features, labels: valid_labels}\n",
    "test_feed_dict = {features: test_features, labels: test_labels}\n",
    "\n",
    "# Linear Function WX + b\n",
    "logits = tf.matmul(features, weights) + biases\n",
    "\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy\n",
    "cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "# Training loss\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Create an operation that initializes all variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Test Cases\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    session.run(loss, feed_dict=train_feed_dict)\n",
    "    session.run(loss, feed_dict=valid_feed_dict)\n",
    "    session.run(loss, feed_dict=test_feed_dict)\n",
    "    biases_data = session.run(biases)\n",
    "\n",
    "assert not np.count_nonzero(biases_data), 'biases must be zeros'\n",
    "\n",
    "print('Tests Passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy function created.\n"
     ]
    }
   ],
   "source": [
    "# Determine if the predictions are correct\n",
    "is_correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "# Calculate the accuracy of the predictions\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n",
    "\n",
    "print('Accuracy function created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/Learn_Rate_Tune_Image.png\" style=\"height: 70%;width: 70%\">\n",
    "## Problem 3\n",
    "Below are 2 parameter configurations for training the neural network. In each configuration, one of the parameters has multiple options. For each configuration, choose the option that gives the best acccuracy.\n",
    "\n",
    "Parameter configurations:\n",
    "\n",
    "Configuration 1\n",
    "* **Epochs:** 1\n",
    "* **Learning Rate:**\n",
    "  * 0.8\n",
    "  * 0.5\n",
    "  * 0.1\n",
    "  * 0.05\n",
    "  * 0.01\n",
    "\n",
    "Configuration 2\n",
    "* **Epochs:**\n",
    "  * 1\n",
    "  * 2\n",
    "  * 3\n",
    "  * 4\n",
    "  * 5\n",
    "* **Learning Rate:** 0.2\n",
    "\n",
    "The code will print out a Loss and Accuracy graph, so you can see how well the neural network performed.\n",
    "\n",
    "*If you're having trouble solving problem 3, you can view the solution [here](intro_to_tensorflow_solution.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  1/5:   0%|          | 0/1114 [00:00<?, ?batches/s]\u001b[A\n",
      "Epoch  1/5:   0%|          | 1/1114 [00:00<13:26,  1.38batches/s]\u001b[A\n",
      "Epoch  1/5:   5%|▍         | 51/1114 [00:01<00:29, 36.06batches/s]\u001b[A\n",
      "Epoch  1/5:   9%|▉         | 101/1114 [00:02<00:20, 48.58batches/s]\u001b[A\n",
      "Epoch  1/5:  14%|█▎        | 151/1114 [00:02<00:17, 55.76batches/s]\u001b[A\n",
      "Epoch  1/5:  18%|█▊        | 201/1114 [00:03<00:15, 60.68batches/s]\u001b[A\n",
      "Epoch  1/5:  23%|██▎       | 251/1114 [00:03<00:13, 63.71batches/s]\u001b[A\n",
      "Epoch  1/5:  27%|██▋       | 301/1114 [00:04<00:12, 66.17batches/s]\u001b[A\n",
      "Epoch  1/5:  32%|███▏      | 351/1114 [00:05<00:11, 66.91batches/s]\u001b[A\n",
      "Epoch  1/5:  36%|███▌      | 401/1114 [00:05<00:10, 68.56batches/s]\u001b[A\n",
      "Epoch  1/5:  40%|████      | 451/1114 [00:06<00:09, 69.54batches/s]\u001b[A\n",
      "Epoch  1/5:  45%|████▍     | 501/1114 [00:07<00:08, 70.41batches/s]\u001b[A\n",
      "Epoch  1/5:  49%|████▉     | 551/1114 [00:07<00:07, 71.32batches/s]\u001b[A\n",
      "Epoch  1/5:  54%|█████▍    | 601/1114 [00:08<00:07, 71.68batches/s]\u001b[A\n",
      "Epoch  1/5:  58%|█████▊    | 651/1114 [00:08<00:06, 72.49batches/s]\u001b[A\n",
      "Epoch  1/5:  63%|██████▎   | 701/1114 [00:09<00:05, 73.20batches/s]\u001b[A\n",
      "Epoch  1/5:  67%|██████▋   | 751/1114 [00:10<00:04, 73.95batches/s]\u001b[A\n",
      "Epoch  1/5:  72%|███████▏  | 801/1114 [00:10<00:04, 74.65batches/s]\u001b[A\n",
      "Epoch  1/5:  76%|███████▋  | 851/1114 [00:11<00:03, 75.26batches/s]\u001b[A\n",
      "Epoch  1/5:  81%|████████  | 901/1114 [00:11<00:02, 75.68batches/s]\u001b[A\n",
      "Epoch  1/5:  85%|████████▌ | 951/1114 [00:12<00:02, 75.91batches/s]\u001b[A\n",
      "Epoch  1/5:  90%|████████▉ | 1001/1114 [00:13<00:01, 75.89batches/s]\u001b[A\n",
      "Epoch  1/5:  94%|█████████▍| 1051/1114 [00:13<00:00, 76.09batches/s]\u001b[A\n",
      "Epoch  1/5:  99%|█████████▉| 1101/1114 [00:14<00:00, 76.48batches/s]\u001b[A\n",
      "Epoch  1/5: 100%|██████████| 1114/1114 [00:14<00:00, 77.28batches/s]\u001b[A\n",
      "Epoch  2/5:   0%|          | 0/1114 [00:00<?, ?batches/s]\u001b[A\n",
      "Epoch  2/5:   0%|          | 1/1114 [00:00<10:02,  1.85batches/s]\u001b[A\n",
      "Epoch  2/5:   5%|▍         | 51/1114 [00:01<00:24, 44.24batches/s]\u001b[A\n",
      "Epoch  2/5:   9%|▉         | 101/1114 [00:01<00:17, 57.76batches/s]\u001b[A\n",
      "Epoch  2/5:  14%|█▎        | 151/1114 [00:02<00:15, 63.07batches/s]\u001b[A\n",
      "Epoch  2/5:  18%|█▊        | 201/1114 [00:03<00:14, 64.81batches/s]\u001b[A\n",
      "Epoch  2/5:  23%|██▎       | 251/1114 [00:03<00:12, 67.97batches/s]\u001b[A\n",
      "Epoch  2/5:  27%|██▋       | 301/1114 [00:04<00:11, 70.10batches/s]\u001b[A\n",
      "Epoch  2/5:  32%|███▏      | 351/1114 [00:04<00:10, 71.56batches/s]\u001b[A\n",
      "Epoch  2/5:  36%|███▌      | 401/1114 [00:05<00:09, 72.19batches/s]\u001b[A\n",
      "Epoch  2/5:  40%|████      | 451/1114 [00:06<00:09, 72.92batches/s]\u001b[A\n",
      "Epoch  2/5:  45%|████▍     | 501/1114 [00:06<00:08, 73.77batches/s]\u001b[A\n",
      "Epoch  2/5:  49%|████▉     | 551/1114 [00:07<00:07, 74.61batches/s]\u001b[A\n",
      "Epoch  2/5:  54%|█████▍    | 601/1114 [00:08<00:06, 74.99batches/s]\u001b[A\n",
      "Epoch  2/5:  58%|█████▊    | 651/1114 [00:08<00:06, 74.43batches/s]\u001b[A\n",
      "Epoch  2/5:  63%|██████▎   | 701/1114 [00:09<00:05, 75.09batches/s]\u001b[A\n",
      "Epoch  2/5:  67%|██████▋   | 751/1114 [00:10<00:04, 74.65batches/s]\u001b[A\n",
      "Epoch  2/5:  72%|███████▏  | 801/1114 [00:10<00:04, 72.98batches/s]\u001b[A\n",
      "Epoch  2/5:  76%|███████▋  | 851/1114 [00:11<00:03, 71.77batches/s]\u001b[A\n",
      "Epoch  2/5:  80%|███████▉  | 889/1114 [00:11<00:03, 74.34batches/s]\u001b[A\n",
      "Epoch  2/5:  81%|████████  | 905/1114 [00:12<00:02, 71.46batches/s]\u001b[A\n",
      "Epoch  2/5:  85%|████████▌ | 951/1114 [00:13<00:02, 71.13batches/s]\u001b[A\n",
      "Epoch  2/5:  90%|████████▉ | 1001/1114 [00:14<00:01, 70.95batches/s]\u001b[A\n",
      "Epoch  2/5:  94%|█████████▍| 1051/1114 [00:14<00:00, 70.63batches/s]\u001b[A\n",
      "Epoch  2/5:  99%|█████████▉| 1101/1114 [00:15<00:00, 70.54batches/s]\u001b[A\n",
      "Epoch  2/5: 100%|██████████| 1114/1114 [00:15<00:00, 71.31batches/s]\u001b[A\n",
      "Epoch  3/5:   0%|          | 0/1114 [00:00<?, ?batches/s]\u001b[A\n",
      "Epoch  3/5:   0%|          | 1/1114 [00:00<11:36,  1.60batches/s]\u001b[A\n",
      "Epoch  3/5:   5%|▍         | 51/1114 [00:01<00:24, 42.90batches/s]\u001b[A\n",
      "Epoch  3/5:   9%|▉         | 101/1114 [00:01<00:17, 57.42batches/s]\u001b[A\n",
      "Epoch  3/5:  14%|█▎        | 151/1114 [00:02<00:14, 64.30batches/s]\u001b[A\n",
      "Epoch  3/5:  18%|█▊        | 201/1114 [00:02<00:13, 67.11batches/s]\u001b[A\n",
      "Epoch  3/5:  23%|██▎       | 251/1114 [00:03<00:12, 70.03batches/s]\u001b[A\n",
      "Epoch  3/5:  27%|██▋       | 301/1114 [00:04<00:11, 72.62batches/s]\u001b[A\n",
      "Epoch  3/5:  32%|███▏      | 351/1114 [00:04<00:10, 74.84batches/s]\u001b[A\n",
      "Epoch  3/5:  36%|███▌      | 401/1114 [00:05<00:09, 76.40batches/s]\u001b[A\n",
      "Epoch  3/5:  40%|████      | 451/1114 [00:05<00:08, 77.88batches/s]\u001b[A\n",
      "Epoch  3/5:  45%|████▍     | 501/1114 [00:06<00:07, 78.69batches/s]\u001b[A\n",
      "Epoch  3/5:  49%|████▉     | 551/1114 [00:06<00:07, 79.17batches/s]\u001b[A\n",
      "Epoch  3/5:  54%|█████▍    | 601/1114 [00:07<00:06, 79.75batches/s]\u001b[A\n",
      "Epoch  3/5:  58%|█████▊    | 651/1114 [00:08<00:05, 80.30batches/s]\u001b[A\n",
      "Epoch  3/5:  63%|██████▎   | 701/1114 [00:08<00:05, 81.03batches/s]\u001b[A\n",
      "Epoch  3/5:  67%|██████▋   | 751/1114 [00:09<00:04, 81.63batches/s]\u001b[A\n",
      "Epoch  3/5:  72%|███████▏  | 801/1114 [00:09<00:03, 82.22batches/s]\u001b[A\n",
      "Epoch  3/5:  76%|███████▋  | 851/1114 [00:10<00:03, 82.76batches/s]\u001b[A\n",
      "Epoch  3/5:  81%|████████  | 901/1114 [00:10<00:02, 83.14batches/s]\u001b[A\n",
      "Epoch  3/5:  85%|████████▌ | 951/1114 [00:11<00:01, 83.51batches/s]\u001b[A\n",
      "Epoch  3/5:  90%|████████▉ | 1001/1114 [00:11<00:01, 83.84batches/s]\u001b[A\n",
      "Epoch  3/5:  94%|█████████▍| 1051/1114 [00:12<00:00, 84.05batches/s]\u001b[A\n",
      "Epoch  3/5:  99%|█████████▉| 1101/1114 [00:13<00:00, 84.01batches/s]\u001b[A\n",
      "Epoch  3/5: 100%|██████████| 1114/1114 [00:13<00:00, 84.91batches/s]\u001b[A\n",
      "Epoch  4/5:   0%|          | 0/1114 [00:00<?, ?batches/s]\u001b[A\n",
      "Epoch  4/5:   0%|          | 1/1114 [00:00<09:18,  1.99batches/s]\u001b[A\n",
      "Epoch  4/5:   5%|▍         | 51/1114 [00:01<00:22, 47.88batches/s]\u001b[A\n",
      "Epoch  4/5:   9%|▉         | 101/1114 [00:01<00:16, 62.38batches/s]\u001b[A\n",
      "Epoch  4/5:  14%|█▎        | 151/1114 [00:02<00:13, 69.86batches/s]\u001b[A\n",
      "Epoch  4/5:  18%|█▊        | 201/1114 [00:02<00:12, 74.21batches/s]\u001b[A\n",
      "Epoch  4/5:  23%|██▎       | 251/1114 [00:03<00:11, 75.95batches/s]\u001b[A\n",
      "Epoch  4/5:  27%|██▋       | 301/1114 [00:03<00:10, 77.29batches/s]\u001b[A\n",
      "Epoch  4/5:  32%|███▏      | 351/1114 [00:04<00:09, 78.19batches/s]\u001b[A\n",
      "Epoch  4/5:  36%|███▌      | 401/1114 [00:05<00:09, 78.11batches/s]\u001b[A\n",
      "Epoch  4/5:  40%|████      | 451/1114 [00:05<00:08, 76.72batches/s]\u001b[A\n",
      "Epoch  4/5:  45%|████▍     | 501/1114 [00:06<00:08, 76.26batches/s]\u001b[A\n",
      "Epoch  4/5:  49%|████▉     | 551/1114 [00:07<00:07, 77.12batches/s]\u001b[A\n",
      "Epoch  4/5:  54%|█████▍    | 601/1114 [00:07<00:06, 76.91batches/s]\u001b[A\n",
      "Epoch  4/5:  58%|█████▊    | 651/1114 [00:08<00:06, 77.08batches/s]\u001b[A\n",
      "Epoch  4/5:  63%|██████▎   | 701/1114 [00:09<00:05, 77.45batches/s]\u001b[A\n",
      "Epoch  4/5:  67%|██████▋   | 751/1114 [00:09<00:04, 77.85batches/s]\u001b[A\n",
      "Epoch  4/5:  72%|███████▏  | 801/1114 [00:10<00:04, 78.01batches/s]\u001b[A\n",
      "Epoch  4/5:  76%|███████▋  | 851/1114 [00:10<00:03, 78.19batches/s]\u001b[A\n",
      "Epoch  4/5:  81%|████████  | 901/1114 [00:11<00:02, 78.47batches/s]\u001b[A\n",
      "Epoch  4/5:  85%|████████▌ | 951/1114 [00:12<00:02, 78.79batches/s]\u001b[A\n",
      "Epoch  4/5:  90%|████████▉ | 1001/1114 [00:12<00:01, 79.01batches/s]\u001b[A\n",
      "Epoch  4/5:  94%|█████████▍| 1051/1114 [00:13<00:00, 79.20batches/s]\u001b[A\n",
      "Epoch  4/5:  99%|█████████▉| 1101/1114 [00:13<00:00, 79.30batches/s]\u001b[A\n",
      "Epoch  4/5: 100%|██████████| 1114/1114 [00:13<00:00, 80.13batches/s]\u001b[A\n",
      "Epoch  5/5:   0%|          | 0/1114 [00:00<?, ?batches/s]\u001b[A\n",
      "Epoch  5/5:   0%|          | 1/1114 [00:00<10:16,  1.81batches/s]\u001b[A\n",
      "Epoch  5/5:   5%|▍         | 51/1114 [00:01<00:24, 43.99batches/s]\u001b[A\n",
      "Epoch  5/5:   9%|▉         | 101/1114 [00:01<00:18, 56.23batches/s]\u001b[A\n",
      "Epoch  5/5:  14%|█▎        | 151/1114 [00:02<00:15, 63.35batches/s]\u001b[A\n",
      "Epoch  5/5:  18%|█▊        | 201/1114 [00:02<00:13, 67.32batches/s]\u001b[A\n",
      "Epoch  5/5:  23%|██▎       | 251/1114 [00:03<00:12, 70.39batches/s]\u001b[A\n",
      "Epoch  5/5:  27%|██▋       | 301/1114 [00:04<00:11, 72.72batches/s]\u001b[A\n",
      "Epoch  5/5:  32%|███▏      | 351/1114 [00:04<00:10, 73.58batches/s]\u001b[A\n",
      "Epoch  5/5:  36%|███▌      | 401/1114 [00:05<00:09, 74.22batches/s]\u001b[A\n",
      "Epoch  5/5:  40%|████      | 451/1114 [00:06<00:08, 75.03batches/s]\u001b[A\n",
      "Epoch  5/5:  45%|████▍     | 501/1114 [00:06<00:08, 75.88batches/s]\u001b[A\n",
      "Epoch  5/5:  49%|████▉     | 551/1114 [00:07<00:07, 76.58batches/s]\u001b[A\n",
      "Epoch  5/5:  54%|█████▍    | 601/1114 [00:07<00:06, 77.05batches/s]\u001b[A\n",
      "Epoch  5/5:  58%|█████▊    | 651/1114 [00:08<00:05, 77.42batches/s]\u001b[A\n",
      "Epoch  5/5:  63%|██████▎   | 701/1114 [00:09<00:05, 77.74batches/s]\u001b[A\n",
      "Epoch  5/5:  67%|██████▋   | 751/1114 [00:09<00:04, 78.22batches/s]\u001b[A\n",
      "Epoch  5/5:  72%|███████▏  | 801/1114 [00:10<00:03, 78.65batches/s]\u001b[A\n",
      "Epoch  5/5:  76%|███████▋  | 851/1114 [00:10<00:03, 79.11batches/s]\u001b[A\n",
      "Epoch  5/5:  81%|████████  | 901/1114 [00:11<00:02, 79.45batches/s]\u001b[A\n",
      "Epoch  5/5:  85%|████████▌ | 951/1114 [00:11<00:02, 79.62batches/s]\u001b[A\n",
      "Epoch  5/5:  90%|████████▉ | 1001/1114 [00:12<00:01, 79.66batches/s]\u001b[A\n",
      "Epoch  5/5:  94%|█████████▍| 1051/1114 [00:13<00:00, 79.79batches/s]\u001b[A\n",
      "Epoch  5/5:  99%|█████████▉| 1101/1114 [00:13<00:00, 79.76batches/s]\u001b[A\n",
      "Epoch  5/5: 100%|██████████| 1114/1114 [00:13<00:00, 80.58batches/s]\u001b[A"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEaCAYAAABEsMO+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1314002b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy at 0.7433333396911621\n"
     ]
    }
   ],
   "source": [
    "# Change if you have memory restrictions\n",
    "batch_size = 128\n",
    "\n",
    "# TODO: Find the best parameters for each configuration\n",
    "epochs = 5\n",
    "learning_rate = 0.3\n",
    "\n",
    "\n",
    "\n",
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)    \n",
    "\n",
    "# The accuracy measured against the validation set\n",
    "validation_accuracy = 0.0\n",
    "\n",
    "# Measurements use for graphing loss and accuracy\n",
    "log_batch_step = 50\n",
    "batches = []\n",
    "loss_batch = []\n",
    "train_acc_batch = []\n",
    "valid_acc_batch = []\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    batch_count = int(math.ceil(len(train_features)/batch_size))\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        \n",
    "        # Progress bar\n",
    "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')\n",
    "        \n",
    "        # The training cycle\n",
    "        for batch_i in batches_pbar:\n",
    "            # Get a batch of training features and labels\n",
    "            batch_start = batch_i*batch_size\n",
    "            batch_features = train_features[batch_start:batch_start + batch_size]\n",
    "            batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "\n",
    "            # Run optimizer and get loss\n",
    "            _, l = session.run(\n",
    "                [optimizer, loss],\n",
    "                feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "            # Log every 50 batches\n",
    "            if not batch_i % log_batch_step:\n",
    "                # Calculate Training and Validation accuracy\n",
    "                training_accuracy = session.run(accuracy, feed_dict=train_feed_dict)\n",
    "                validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)\n",
    "\n",
    "                # Log batches\n",
    "                previous_batch = batches[-1] if batches else 0\n",
    "                batches.append(log_batch_step + previous_batch)\n",
    "                loss_batch.append(l)\n",
    "                train_acc_batch.append(training_accuracy)\n",
    "                valid_acc_batch.append(validation_accuracy)\n",
    "\n",
    "        # Check accuracy against Validation data\n",
    "        validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)\n",
    "\n",
    "loss_plot = plt.subplot(211)\n",
    "loss_plot.set_title('Loss')\n",
    "loss_plot.plot(batches, loss_batch, 'g')\n",
    "loss_plot.set_xlim([batches[0], batches[-1]])\n",
    "acc_plot = plt.subplot(212)\n",
    "acc_plot.set_title('Accuracy')\n",
    "acc_plot.plot(batches, train_acc_batch, 'r', label='Training Accuracy')\n",
    "acc_plot.plot(batches, valid_acc_batch, 'x', label='Validation Accuracy')\n",
    "acc_plot.set_ylim([0, 1.0])\n",
    "acc_plot.set_xlim([batches[0], batches[-1]])\n",
    "acc_plot.legend(loc=4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Validation accuracy at {}'.format(validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "You're going to test your model against your hold out dataset/testing data.  This will give you a good indicator of how well the model will do in the real world.  You should have a test accuracy of at least 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  1/5:   0%|          | 0/1114 [00:00<?, ?batches/s]\u001b[A\n",
      "Epoch  1/5:   5%|▌         | 59/1114 [00:00<00:01, 582.89batches/s]\u001b[A\n",
      "Epoch  1/5:  16%|█▌        | 181/1114 [00:00<00:01, 898.82batches/s]\u001b[A\n",
      "Epoch  1/5:  27%|██▋       | 305/1114 [00:00<00:00, 1010.69batches/s]\u001b[A\n",
      "Epoch  1/5:  38%|███▊      | 424/1114 [00:00<00:00, 1054.51batches/s]\u001b[A\n",
      "Epoch  1/5:  49%|████▉     | 546/1114 [00:00<00:00, 1086.50batches/s]\u001b[A\n",
      "Epoch  1/5:  60%|██████    | 669/1114 [00:00<00:00, 1109.09batches/s]\u001b[A\n",
      "Epoch  1/5:  71%|███████   | 791/1114 [00:00<00:00, 1124.06batches/s]\u001b[A\n",
      "Epoch  1/5:  82%|████████▏ | 913/1114 [00:00<00:00, 1135.70batches/s]\u001b[A\n",
      "Epoch  1/5:  93%|█████████▎| 1038/1114 [00:00<00:00, 1147.46batches/s]\u001b[A\n",
      "Epoch  1/5: 100%|██████████| 1114/1114 [00:00<00:00, 1141.77batches/s]\u001b[A\n",
      "Epoch  2/5:   0%|          | 0/1114 [00:00<?, ?batches/s]\u001b[A\n",
      "Epoch  2/5:  11%|█         | 121/1114 [00:00<00:00, 1203.38batches/s]\u001b[A\n",
      "Epoch  2/5:  22%|██▏       | 243/1114 [00:00<00:00, 1207.90batches/s]\u001b[A\n",
      "Epoch  2/5:  32%|███▏      | 362/1114 [00:00<00:00, 1201.58batches/s]\u001b[A\n",
      "Epoch  2/5:  43%|████▎     | 479/1114 [00:00<00:00, 1191.71batches/s]\u001b[A\n",
      "Epoch  2/5:  53%|█████▎    | 585/1114 [00:00<00:00, 1164.87batches/s]\u001b[A\n",
      "Epoch  2/5:  62%|██████▏   | 692/1114 [00:00<00:00, 1148.81batches/s]\u001b[A\n",
      "Epoch  2/5:  72%|███████▏  | 797/1114 [00:00<00:00, 1134.09batches/s]\u001b[A\n",
      "Epoch  2/5:  81%|████████  | 898/1114 [00:00<00:00, 1116.86batches/s]\u001b[A\n",
      "Epoch  2/5:  90%|████████▉ | 1001/1114 [00:00<00:00, 1107.09batches/s]\u001b[A\n",
      "Epoch  2/5:  99%|█████████▉| 1106/1114 [00:01<00:00, 1099.76batches/s]\u001b[A\n",
      "Epoch  2/5: 100%|██████████| 1114/1114 [00:01<00:00, 1096.32batches/s]\u001b[A\n",
      "Epoch  3/5:   0%|          | 0/1114 [00:00<?, ?batches/s]\u001b[A\n",
      "Epoch  3/5:   8%|▊         | 92/1114 [00:00<00:01, 913.62batches/s]\u001b[A\n",
      "Epoch  3/5:  18%|█▊        | 204/1114 [00:00<00:00, 1014.91batches/s]\u001b[A\n",
      "Epoch  3/5:  29%|██▉       | 327/1114 [00:00<00:00, 1084.49batches/s]\u001b[A\n",
      "Epoch  3/5:  38%|███▊      | 428/1114 [00:00<00:00, 1064.00batches/s]\u001b[A\n",
      "Epoch  3/5:  46%|████▌     | 513/1114 [00:00<00:00, 1020.07batches/s]\u001b[A\n",
      "Epoch  3/5:  54%|█████▍    | 599/1114 [00:00<00:00, 993.64batches/s] \u001b[A\n",
      "Epoch  3/5:  61%|██████▏   | 683/1114 [00:00<00:00, 951.57batches/s]\u001b[A\n",
      "Epoch  3/5:  71%|███████   | 788/1114 [00:00<00:00, 963.90batches/s]\u001b[A\n",
      "Epoch  3/5:  81%|████████  | 902/1114 [00:00<00:00, 982.39batches/s]\u001b[A\n",
      "Epoch  3/5:  92%|█████████▏| 1021/1114 [00:01<00:00, 1002.81batches/s]\u001b[A\n",
      "Epoch  3/5: 100%|██████████| 1114/1114 [00:01<00:00, 1014.25batches/s]\u001b[A\n",
      "Epoch  4/5:   0%|          | 0/1114 [00:00<?, ?batches/s]\u001b[A\n",
      "Epoch  4/5:  11%|█         | 117/1114 [00:00<00:00, 1158.58batches/s]\u001b[A\n",
      "Epoch  4/5:  21%|██        | 231/1114 [00:00<00:00, 1146.74batches/s]\u001b[A\n",
      "Epoch  4/5:  31%|███       | 341/1114 [00:00<00:00, 1128.70batches/s]\u001b[A\n",
      "Epoch  4/5:  38%|███▊      | 425/1114 [00:00<00:00, 1054.13batches/s]\u001b[A\n",
      "Epoch  4/5:  46%|████▌     | 515/1114 [00:00<00:00, 1022.47batches/s]\u001b[A\n",
      "Epoch  4/5:  55%|█████▌    | 617/1114 [00:00<00:00, 1021.67batches/s]\u001b[A\n",
      "Epoch  4/5:  66%|██████▌   | 730/1114 [00:00<00:00, 1036.84batches/s]\u001b[A\n",
      "Epoch  4/5:  76%|███████▌  | 845/1114 [00:00<00:00, 1050.59batches/s]\u001b[A\n",
      "Epoch  4/5:  85%|████████▌ | 951/1114 [00:00<00:00, 1050.30batches/s]\u001b[A\n",
      "Epoch  4/5:  96%|█████████▌| 1070/1114 [00:01<00:00, 1064.24batches/s]\u001b[A\n",
      "Epoch  4/5: 100%|██████████| 1114/1114 [00:01<00:00, 1062.69batches/s]\u001b[A\n",
      "Epoch  5/5:   0%|          | 0/1114 [00:00<?, ?batches/s]\u001b[A\n",
      "Epoch  5/5:   7%|▋         | 78/1114 [00:00<00:01, 773.70batches/s]\u001b[A\n",
      "Epoch  5/5:  18%|█▊        | 199/1114 [00:00<00:00, 991.58batches/s]\u001b[A\n",
      "Epoch  5/5:  29%|██▉       | 326/1114 [00:00<00:00, 1084.02batches/s]\u001b[A\n",
      "Epoch  5/5:  41%|████      | 453/1114 [00:00<00:00, 1129.90batches/s]\u001b[A\n",
      "Epoch  5/5:  50%|████▉     | 555/1114 [00:00<00:00, 1104.87batches/s]\u001b[A\n",
      "Epoch  5/5:  59%|█████▊    | 652/1114 [00:00<00:00, 1081.05batches/s]\u001b[A\n",
      "Epoch  5/5:  67%|██████▋   | 747/1114 [00:00<00:00, 1058.83batches/s]\u001b[A\n",
      "Epoch  5/5:  76%|███████▌  | 845/1114 [00:00<00:00, 1048.86batches/s]\u001b[A\n",
      "Epoch  5/5:  86%|████████▌ | 958/1114 [00:00<00:00, 1057.39batches/s]\u001b[A\n",
      "Epoch  5/5:  96%|█████████▋| 1073/1114 [00:01<00:00, 1065.99batches/s]\u001b[A\n",
      "Epoch  5/5: 100%|██████████| 1114/1114 [00:01<00:00, 1062.60batches/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice Job! Test Accuracy is 0.803600013256073\n"
     ]
    }
   ],
   "source": [
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "# The accuracy measured against the test set\n",
    "test_accuracy = 0.0\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    session.run(init)\n",
    "    batch_count = int(math.ceil(len(train_features)/batch_size))\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        \n",
    "        # Progress bar\n",
    "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')\n",
    "        \n",
    "        # The training cycle\n",
    "        for batch_i in batches_pbar:\n",
    "            # Get a batch of training features and labels\n",
    "            batch_start = batch_i*batch_size\n",
    "            batch_features = train_features[batch_start:batch_start + batch_size]\n",
    "            batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "\n",
    "            # Run optimizer\n",
    "            _ = session.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "        # Check accuracy against Test data\n",
    "        test_accuracy = session.run(accuracy, feed_dict=test_feed_dict)\n",
    "\n",
    "\n",
    "assert test_accuracy >= 0.80, 'Test accuracy at {}, should be equal to or greater than 0.80'.format(test_accuracy)\n",
    "print('Nice Job! Test Accuracy is {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple layers\n",
    "Good job!  You built a one layer TensorFlow network!  However, you might want to build more than one layer.  This is deep learning after all!  In the next section, you will start to satisfy your need for more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.5.2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python3/3.5.2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python3/3.5.2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python3/3.5.2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python3/3.5.2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python3/3.5.2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python3/3.5.2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python3/3.5.2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python3/3.5.2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print('All modules imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data and modules loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reload the data\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  pickle_data = pickle.load(f)\n",
    "  train_features = pickle_data['train_dataset']\n",
    "  train_labels = pickle_data['train_labels']\n",
    "  valid_features = pickle_data['valid_dataset']\n",
    "  valid_labels = pickle_data['valid_labels']\n",
    "  test_features = pickle_data['test_dataset']\n",
    "  test_labels = pickle_data['test_labels']\n",
    "  del pickle_data  # Free up memory\n",
    "\n",
    "print('Data and modules loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    # TODO: Implement batching\n",
    "    \n",
    "    batches = []\n",
    "    sample_size = len(features)\n",
    "    for i in range(0, sample_size, batch_size):\n",
    "        end = i + batch_size\n",
    "        batch = [features[i: end], labels[i: end] ]\n",
    "        batches.append(batch)\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(256, 10)\n"
     ]
    }
   ],
   "source": [
    "# All the pixels in the image (28 * 28 = 784)\n",
    "features_count = 784\n",
    "h1_node = 256\n",
    "# All the labels\n",
    "labels_count = 10\n",
    "\n",
    "# TODO: Set the features and labels tensors\n",
    "features = tf.placeholder(\"float\", [None, features_count])\n",
    "Y = tf.placeholder(\"float\", [None, labels_count]) \n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "print(type(learning_rate))\n",
    "# TODO: Set the weights and biases tensors\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([features_count, h1_node])),\n",
    "    'out': tf.Variable(tf.random_normal([h1_node, labels_count]))\n",
    "}\n",
    "\n",
    "print(weights['out'].shape)\n",
    "\n",
    "biases = {\n",
    "    'h1': tf.Variable(tf.random_normal([h1_node])),\n",
    "    'out': tf.Variable(tf.random_normal([labels_count]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_feed_dict={\n",
    "                features: valid_features,\n",
    "                Y: valid_labels,\n",
    "    }\n",
    "\n",
    "test_feed_dict={\n",
    "                features: test_features,\n",
    "                Y: test_labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hidden layer with RELU activation\n",
    "h1_layer = tf.add(tf.matmul(features, weights['h1']), biases['h1'])\n",
    "h1_output = tf.nn.relu(h1_layer)\n",
    "\n",
    "# Output layer with linear activation\n",
    "logits = tf.add(tf.matmul(h1_output, weights['out']), biases['out'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy function created.\n"
     ]
    }
   ],
   "source": [
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Determine if the predictions are correct\n",
    "is_correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "# Calculate the accuracy of the predictions\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n",
    "\n",
    "print('Accuracy function created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch:  0\n",
      "End of Epoch:  1\n",
      "End of Epoch:  2\n",
      "End of Epoch:  3\n",
      "End of Epoch:  4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEaCAYAAABEsMO+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUCIiEpH+H10tp0s1Zq2lAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c05e9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy at 0.7883999943733215\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: Find the best parameters for each configuration\n",
    "epochs = 5\n",
    "lr = 0.3\n",
    "batch_size = 128\n",
    "\n",
    "log_batch_step = 100\n",
    "batches = []\n",
    "loss_batch = []\n",
    "train_acc_batch = []\n",
    "valid_acc_batch = []\n",
    "\n",
    "\n",
    "# The file path to save the data\n",
    "save_file = './model/first_model.ckpt'\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)    \n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "\n",
    "    for epoch_i in range(epochs):   \n",
    "        b = 0\n",
    "        batches_data = create_batches(batch_size, train_features, train_labels)\n",
    "\n",
    "        # The training cycle\n",
    "        for batch_features, batch_labels in batches_data:\n",
    "            b += 1\n",
    "                        \n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                Y: batch_labels,\n",
    "                learning_rate:lr\n",
    "                }\n",
    "\n",
    "            session.run(optimizer, feed_dict=train_feed_dict)\n",
    "            \n",
    "            if not b % log_batch_step:\n",
    "                    # Calculate Training and Validation accuracy\n",
    "                    training_accuracy = session.run(accuracy, feed_dict=train_feed_dict)\n",
    "                    validation_accuracy = session.run(accuracy, feed_dict=val_feed_dict)\n",
    "\n",
    "                    # Log batches\n",
    "                    previous_batch = batches[-1] if batches else 0\n",
    "                    batches.append(log_batch_step + previous_batch)\n",
    "                    train_acc_batch.append(training_accuracy)\n",
    "                    valid_acc_batch.append(validation_accuracy)\n",
    "\n",
    "                    \n",
    "        validation_accuracy = session.run(accuracy, feed_dict=val_feed_dict)\n",
    "        print(\"End of Epoch: \", epoch_i)\n",
    "\n",
    "    saver.save(session, save_file)\n",
    "    \n",
    "        \n",
    "loss_plot = plt.subplot(211)\n",
    "\n",
    "acc_plot = plt.subplot(212)\n",
    "acc_plot.set_title('Accuracy')\n",
    "\n",
    "acc_plot.plot(batches, train_acc_batch, 'r', label='Training Accuracy')\n",
    "acc_plot.plot(batches, valid_acc_batch, 'x', label='Validation Accuracy')\n",
    "acc_plot.set_ylim([0, 1.0])\n",
    "acc_plot.set_xlim([batches[0], batches[-1]])\n",
    "acc_plot.legend(loc=4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Validation accuracy at {}'.format(validation_accuracy))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8613\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy against Test data\n",
    "save_file = './model/first_model.ckpt'\n",
    "saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "#     sess.run(init)\n",
    "    test_accuracy = sess.run(accuracy, feed_dict=test_feed_dict)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Running in multi-layer perceptron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model multi-layer perceptron is a over kill and the model is not learning anything. The accuracy is 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(128, 10)\n"
     ]
    }
   ],
   "source": [
    "# All the pixels in the image (28 * 28 = 784)\n",
    "features_count = 784\n",
    "h1_node = 256\n",
    "h2_node = 128\n",
    "# All the labels\n",
    "labels_count = 10\n",
    "\n",
    "# TODO: Set the features and labels tensors\n",
    "features = tf.placeholder(\"float\", [None, features_count])\n",
    "Y = tf.placeholder(\"float\", [None, labels_count]) \n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "print(type(learning_rate))\n",
    "# TODO: Set the weights and biases tensors\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([features_count, h1_node])),\n",
    "    'h2': tf.Variable(tf.random_normal([h1_node, h2_node])),\n",
    "    'out': tf.Variable(tf.random_normal([h2_node, labels_count]))\n",
    "}\n",
    "\n",
    "print(weights['out'].shape)\n",
    "\n",
    "biases = {\n",
    "    'h1': tf.Variable(tf.random_normal([h1_node])),\n",
    "    'h2': tf.Variable(tf.random_normal([h2_node])),\n",
    "    'out': tf.Variable(tf.random_normal([labels_count]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'h2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1e8e58401185>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mh1_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mh1_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mh2_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mh2_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Output layer with linear activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'h2'"
     ]
    }
   ],
   "source": [
    "# Hidden layer with RELU activation\n",
    "h1_layer = tf.add(tf.matmul(features, weights['h1']), biases['h1'])\n",
    "h1_output = tf.nn.relu(h1_layer)\n",
    "h2_layer = tf.add(tf.matmul(h1_output, weights['h2']), biases['h2'])\n",
    "h2_output = tf.nn.relu(h2_layer)\n",
    "# Output layer with linear activation\n",
    "logits = tf.add(tf.matmul(h2_output, weights['out']), biases['out'])\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
